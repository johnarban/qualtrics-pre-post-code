# Generated by Copilot
from importlib import reload

import csv
import json
import regex
from io import StringIO


import numpy as np
import pandas as pd


from qualtrics_keys import *
import qualtrics_api as qapi
import nlp_summary

import get_class_from_db as db
from scipy.stats import binomtest

reload(db)
reload(qapi)
reload(nlp_summary)

pd.set_option("display.max_colwidth", None)


def clean_up_description(description: str):
    """
    Cleans up survey question descriptions by removing newlines, non-ascii characters, and extra spaces.
    """
    # remove new line characters
    description = description.replace("\n", "")
    # remove non-ascii characters
    description = description.encode("ascii", "ignore").decode("ascii")
    # remove runs of spaces
    description = regex.sub(r"\s+", " ", description)
    # remove leading and trailing whitespace
    description = description.strip()
    return description


def read_in_qualtrics(f: StringIO):
    """
    Reads a Qualtrics CSV file and returns header, description, formatspec, and data as numpy array.
    """
    reader = csv.reader(f, delimiter=",", quotechar='"')
    header = next(reader)
    description = next(reader)
    description = [clean_up_description(d) for d in description]
    formatspec = next(reader)
    data = [row for row in reader]
    return header, description, formatspec, np.asarray(data)


columns_to_drop = [
    "Status",
    "StartDate",
    "EndDate",
    "Finished",
    "IPAddress",
    "Progress",
    "RecordedDate",
    "ResponseId",
    "RecipientLastName",
    "RecipientFirstName",
    "RecipientEmail",
    "ExternalReference",
    "LocationLatitude",
    "LocationLongitude",
    "DistributionChannel",
    "UserLanguage",
    "Intro information_2",
    "Intro information_3",
]


def load_data(from_api=True, survey_id="", filename=None):
    """
    Loads survey data from API or local CSV file. Writes API data to file if fetched.
    """
    if filename is None:
        filename = survey_id + "_response.csv"

    if from_api:
        pre_response = qapi.get_survey(
            token, data_center, survey_id, format="csv", numeric=False
        )
        if pre_response is not None:
            with open(filename, "w") as f:
                f.write(pre_response)
        return pre_response

    if not from_api:
        with open(filename, "r") as f:
            return f.read()


def load_pre_data(from_api=True, filename="2025_pre_response.csv"):
    """
    Loads pre-survey data using load_data().
    """
    return load_data(from_api, survey_id_2025_pre, filename)


def load_post_data(from_api=True, filename="2025_post_response.csv"):
    """
    Loads post-survey data using load_data().
    """
    return load_data(from_api, survey_id_2025_post, filename)


def parse_response(response):
    """
    Parses a Qualtrics CSV response string into header, description, and data.
    """
    with StringIO(response) as f:
        tuple = read_in_qualtrics(f)
        header, description, _, data = tuple

    assert len(header) == len(
        description
    ), f"Header and description lengths do not match: {len(header)} != {len(description)}"
    assert len(header) == len(
        data[0]
    ), f"Header and data lengths do not match: {len(header)} != {len(data[0])}"

    return header, description, data


def column_cleanup(
    header_pre,
    description_pre,
    data_pre,
    header_post,
    description_post,
    data_post,
    drop_columns=[],
):
    """
    Drops specified columns from pre and post survey data and aligns question descriptions.
    """
    for i in range(len(description_post)):
        if "Post-Survey" in description_post[i]:
            description_post[i] = description_post[i].replace(
                "Post-Survey", "Pre-Survey"
            )

    for col in drop_columns:
        if col in description_post:
            post_idx = description_post.index(col)
            print(f"Dropping column {col} from post survey")
            header_post.pop(post_idx)
            description_post.pop(post_idx)
            data_post = np.delete(data_post, post_idx, axis=1)
        if col in description_pre:
            pre_idx = description_pre.index(col)
            print(f"Dropping column {col} from pre survey")
            header_pre.pop(pre_idx)
            description_pre.pop(pre_idx)
            data_pre = np.delete(data_pre, pre_idx, axis=1)


def get_pre_header_for_post_question(
    question, header_pre, description_pre, header_post, description_post, verbose=False
):
    """
    Ensures that matching questions in pre and post have the same header tag.
    """
    post_idx = description_post.index(question)
    post_header = header_post[post_idx]

    try:
        pre_idx = description_pre.index(question)
    except ValueError:
        if not verbose:
            return
        print(f"Question {question} not found in pre survey")
        return

    pre_header = header_pre[pre_idx]
    if (description_post[post_idx] == description_pre[pre_idx]) and (
        header_post[post_idx] == header_pre[pre_idx]
    ):
        if not verbose:
            return
        print(f"No need to change header {post_header} for question")
        return

    if pre_header not in header_post:
        # print(f"Changing header {post_header} to {pre_header}")
        header_post[post_idx] = pre_header
    else:
        print(f"Header {pre_header} already in post survey")
        pre_idx = header_post.index(pre_header)
        post_idx = header_post.index(pre_header)
        pre_q = description_pre[pre_idx]
        post_q = description_post[post_idx]
        print(f"Pre: {pre_header} Question: {pre_q}")
        print(f"Post: {pre_header} Question: {post_q}")


def create_initial_dataframes(
    header_pre,
    description_pre,
    data_pre,
    header_post,
    description_post,
    data_post,
    id_column,
):
    """
    Creates pandas DataFrames for pre and post survey data, indexed by id_column. Drops duplicate IDs in pre.
    """
    df_pre = pd.DataFrame(data_pre, columns=description_pre)
    df_post = pd.DataFrame(data_post, columns=description_post)

    if not df_pre[id_column].is_unique:
        duplicates = df_pre[df_pre.duplicated(subset=[id_column], keep=False)][
            id_column
        ].unique()
        print("Renaming duplicate ids in pre:")
        for dup in duplicates:
            dup_indices = df_pre[df_pre[id_column] == dup].index
            for i, idx in enumerate(dup_indices):
                suffix = f"_pre_{hex(i)[2:].zfill(2)}"
                new_id = f"{dup}{suffix}"
                print(f"\t{dup} -> {new_id}")
                df_pre.at[idx, id_column] = new_id

    if not df_post[id_column].is_unique:
        duplicates = df_post[df_post.duplicated(subset=[id_column], keep=False)][
            id_column
        ].unique()
        print("Renaming duplicate ids in post:")
        for dup in duplicates:
            dup_indices = df_post[df_post[id_column] == dup].index
            for i, idx in enumerate(dup_indices):
                suffix = f"_post_{hex(i)[2:].zfill(2)}"
                new_id = f"{dup}{suffix}"
                print(f"\t{dup} -> {new_id}")
                df_post.at[idx, id_column] = new_id

    return df_pre, df_post


def create_question_dataframe(
    df_pre, df_post, header_pre, description_pre, header_post, description_post
):
    """
    Builds a DataFrame listing all questions, their presence in pre/post, and their tags.
    """
    q = [c for c in df_pre.columns]
    _questions = q + [c for c in df_post.columns if c not in q]

    questions_in_common = [
        c in df_post.columns and c in df_pre.columns for c in _questions
    ]
    questions_in_pre = [c in df_pre.columns for c in _questions]
    questions_in_post = [c in df_post.columns for c in _questions]
    tag_pre = [
        header_pre[description_pre.index(q)] if q in description_pre else None
        for q in _questions
    ]
    tag_post = [
        header_post[description_post.index(q)] if q in description_post else None
        for q in _questions
    ]
    print(f"Questions in common: {sum(questions_in_common)}")
    print(f"Questions in pre: {sum(questions_in_pre)}")
    print(f"Questions in post: {sum(questions_in_post)}")

    return pd.DataFrame(
        {
            "question": _questions,
            "both": questions_in_common,
            "in_pre": questions_in_pre,
            "in_post": questions_in_post,
            "tag_pre": tag_pre,
            "tag_post": tag_post,
            "question_category": [None] * len(_questions),
        },
    )


def is_likert_answer(x):
    """
    Returns True if the answer string is a Likert scale response.
    """
    if isinstance(x, str):
        return any(
            [
                "strongly disagree" in x.lower(),
                "strongly agree" in x.lower(),
            ]
        )
    return False


def is_likert_column(x: pd.Series):
    """
    Returns True if any value in the Series is a Likert answer.
    """
    return x.apply(is_likert_answer).any()


def convert_likert_to_numeric(x):
    """
    Converts a Likert response to a numeric value (if possible).
    """
    if len(x) == 0:
        return np.nan
    return int(x[0])


def add_likert_columns(questions, df_pre, df_post):
    """
    Adds a boolean column to questions DataFrame indicating Likert questions.
    """
    likert_columns = set(df_pre.columns[df_pre.apply(is_likert_column)]).union(
        set(df_post.columns[df_post.apply(is_likert_column)])
    )
    questions["is_likert"] = questions["question"].apply(lambda x: x in likert_columns)
    questions.loc[questions["is_likert"], "question_category"] = "likert"


# We need to reconstruct the parent demographic question. The API returns this split accross


def process_parent_questions(questions, df_pre, df_post, sep=2):
    """
    Combines parent demographic questions split across multiple columns into single columns.
    """
    # identify the parent education and gender questions.
    parent1_edu = questions.loc[
        questions["question"].str.contains("Parent/Guardian 1")
    ]["question"][sep:]
    parent1_gen = questions.loc[
        questions["question"].str.contains("Parent/Guardian 1")
    ]["question"][:sep]

    parent2_edu = questions.loc[
        questions["question"].str.contains("Parent/Guardian 2")
    ]["question"][sep:]
    parent2_gen = questions.loc[
        questions["question"].str.contains("Parent/Guardian 2")
    ]["question"][:sep]

    education_map = {
        "": 0,
        "Bachelor’s Degree": 1,
        "Less than High School": 2,
        "High School Diploma/GED": 3,
        "Some College/ Associate Degree": 4,
        "Master’s Degree or higher": 5,
    }
    # get the highest education level of the parent
    df_pre["Parent 1 Education"] = df_pre[parent1_edu].apply(
        lambda x: max(x, key=lambda i: education_map.get(i, 0)) or "(empty)", axis=1
    )
    df_pre["Parent 2 Education"] = df_pre[parent2_edu].apply(
        lambda x: max(x, key=lambda i: education_map.get(i, 0)) or "(empty)", axis=1
    )
    df_pre["Parent 1 Gender"] = df_pre[parent1_gen].apply(
        lambda x: max(x) or "(empty)", axis=1
    )

    df_pre["Parent 2 Gender"] = df_pre[parent2_gen].apply(
        lambda x: max(x) or "(empty)", axis=1
    )

    # add these to the questions
    new_questions = [
        {
            "question": "Parent 1 Education",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Parent/Guardian 1")
            ]["tag_pre"][sep:].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
        {
            "question": "Parent 1 Gender",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Parent/Guardian 1")
            ]["tag_pre"][:sep].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
        {
            "question": "Parent 2 Education",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Parent/Guardian 2")
            ]["tag_pre"][sep:].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
        {
            "question": "Parent 2 Gender",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Parent/Guardian 2")
            ]["tag_pre"][:sep].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]
    # remove the old questions
    questions = questions.loc[~questions["question"].isin(parent1_edu)]
    questions = questions.loc[~questions["question"].isin(parent2_edu)]
    questions = questions.loc[~questions["question"].isin(parent1_gen)]
    questions = questions.loc[~questions["question"].isin(parent2_gen)]
    # add the new questions
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return (
        questions,
        parent1_edu.tolist()
        + parent2_edu.tolist()
        + parent1_gen.tolist()
        + parent2_gen.tolist(),
    )


def process_race_questions(questions, df_pre, df_post):
    """
    Combines race questions into a single 'Student Race' column.
    """
    race_questions = questions.loc[
        questions["question"].str.contains("With which race do you identify")
    ]["question"]

    df_pre["Student Race"] = df_pre[race_questions].apply(
        lambda v: frozenset(filter(bool, v)), axis=1
    ).apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))

    new_questions = [
        {
            "question": "Student Race",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("With which race do you identify")
            ]["tag_pre"].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(race_questions)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, race_questions


def process_hispanic_questions(questions, df_pre, df_post):
    """
    Combines Hispanic origin questions into a single 'Student Hispanic Origin' column.
    """
    hispanic_questions = questions.loc[
        questions["question"].str.contains("Hispanic")
    ]["question"].values[0]

    def translate(x):
        if x == "Yes":
            return "Hispanic"
        elif x == "No":
            return "Not Hispanic"
        else:
            return x

    df_pre["Hispanic"] = df_pre[hispanic_questions]
    #     lambda x: translate(x).strip()
    # ).apply(
    #     lambda x: x if len(x) > 0 else frozenset(["(empty)"])
    # )

    new_questions = [
        {
            "question": "Hispanic",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Hispanic")
            ]["tag_pre"].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin([hispanic_questions])]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, hispanic_questions

# 19. With which gender do you identify? - Prefer to Self Describe - Text', '19. With which gender do you identify? - Selected Choice'
def process_gender_questions(questions, df_pre, df_post):
    """
    Combines gender identity questions into a single 'Student Gender' column.
    """
    # combine
    gender_questions = questions.loc[
        questions["question"].str.contains("With which gender do you identify")
    ]["question"]

    df_pre["Student Gender"] = (
        df_pre[gender_questions]
        .apply(lambda v: frozenset(filter(bool, v)), axis=1)
        .apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))
    )

    new_questions = [
        {
            "question": "Student Gender",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("With which gender do you identify")
            ]["tag_pre"].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(gender_questions)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, gender_questions


def process_school_questions(questions, df_pre, df_post):  # school questions
    """
    Combines school level questions into a single 'Student School Level' column.
    """
    # combine the race questions
    school_questions = questions.loc[questions["question"].str.contains("Are you in")][
        "question"
    ]

    # what magic is frozenset(filter(bool,v)) doing? it is removing all the empty strings from the list
    df_pre["Student School Level"] = (
        df_pre[school_questions]
        .apply(lambda v: frozenset(filter(bool, v)), axis=1)
        .apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))
    )

    new_questions = [
        {
            "question": "Student School Level",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[questions["question"].str.contains("Are you in")][
                "tag_pre"
            ].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(school_questions)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, school_questions


def process_activity_questions(questions, df_pre, df_post):
    """
    Combines activity reflection questions into a single 'Favorite Activity' column.
    """
    # school questions
    # combine the race questions
    # escape cuz it treats it like a regex
    activity_question = questions.loc[
        questions["question"].str.contains("Which activity \(or activities\) did you")
    ]["question"]

    # what magic is frozenset(filter(bool,v)) doing? it is removing all the empty strings from the list
    df_post["Favorite Activitiy"] = (
        df_post[activity_question]
        .apply(lambda v: frozenset(filter(bool, v)), axis=1)
        .apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))
    )

    new_questions = [
        {
            "question": "Favorite Activitiy",
            "both": False,
            "in_pre": False,
            "in_post": True,
            "tag_pre": questions.loc[
                questions["question"].str.contains(
                    "Which activity \(or activities\) did you"
                )
            ]["tag_post"].values[0],
            "tag_post": None,
            "question_category": "reflection",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(activity_question)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, activity_question


def process_confused_questions(questions, df_pre, df_post):
    """
    Combines 'most difficult or confusing activity' questions into a single column.
    """
    # school questions
    # combine the race questions
    # escape cuz it treats it like a regex
    _question = questions.loc[
        questions["question"].str.contains(
            "Which activity was the most difficult or confusing"
        )
    ]["question"]

    # what magic is frozenset(filter(bool,v)) doing? it is removing all the empty strings from the list
    df_post["Which activity was the most difficult or confusing"] = (
        df_post[_question]
        .apply(lambda v: frozenset(filter(bool, v)), axis=1)
        .apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))
    )

    new_questions = [
        {
            "question": "Which activity was the most difficult or confusing",
            "both": False,
            "in_pre": False,
            "in_post": True,
            "tag_pre": questions.loc[
                questions["question"].str.contains(
                    "Which activity was the most difficult or confusing"
                )
            ]["tag_post"].values[0],
            "tag_post": None,
            "question_category": "reflection",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(_question)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, _question


def process_esl_questions(questions, df_pre, df_post):
    """
    Combines ESL (English as a Second Language) questions into a single column.
    """
    # school questions
    # combine
    esl_questions = questions.loc[
        questions["question"].str.contains("Is English the primary spoken")
    ]["question"]

    def isESL(val):
        if val == "No":
            return "ESL"
        elif val == "Yes":
            return "Not ESL"
        elif val == "":
            return "(empty)"
        return val

    df_pre["Student ESL"] = df_pre[esl_questions].apply(
        lambda x: next((isESL(i) for i in x), ("(empty)")), axis=1
    )

    new_questions = [
        {
            "question": "Student ESL",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Is English the primary spoken")
            ]["tag_pre"].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]
    questions = questions.loc[~questions["question"].isin(esl_questions)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)
    return questions, esl_questions


def process_likert_columns(df, questions):
    """
    Converts Likert columns in the DataFrame to numeric values using convert_likert_to_numeric().
    """
    # convert the likert columns to numeric
    for col in df.columns:
        if col in questions["question"].values:
            if questions.loc[questions["question"] == col]["is_likert"].values[0]:
                df[col] = df[col].apply(lambda x: convert_likert_to_numeric(x))
    return df


def add_content_question_meta(questions, description_pre, description_post, quiet=True):
    """
    Adds metadata (answers, groups, summaries) to the questions DataFrame from qa_pairs.json.
    """
    """All changeds are don in place"""
    with open("qa_pairs.json", "r") as f:
        qa_pairs = json.load(f)

    if not quiet:
        print(f"There are {len(qa_pairs)} QA pairs")

    questions_and_answers = {q["question"]: q["answer"] for q in qa_pairs}

    # make sure every q & a is in description_pre_tex and description_post
    for w in qa_pairs:
        question = w["question"]
        if (question not in description_pre) and (question not in description_post):
            if not quiet:
                print(f"Removing question {question} from qa_pairs")
            del questions_and_answers[question]

    # Build a mapping from question to groups
    question_to_groups = {q["question"]: q.get("group", []) for q in qa_pairs}
    questions["groups"] = (
        questions["question"]
        .map(question_to_groups)
        .apply(lambda x: frozenset(x) if str(x) != "nan" else frozenset([]))
    )

    # in questions add an empty column for the answer and the short version of the question
    questions["answer"] = None
    questions["short_question"] = None

    # for questions where is_likert is true, set the short question to the nlp_summary.create_summary
    questions["short_question"] = questions["question"].apply(
        lambda x: nlp_summary.create_summary(x.split("? -", maxsplit=1)[-1].strip(), 6)
    )

    questions["answer"] = questions["question"].apply(
        lambda x: questions_and_answers[x] if x in questions_and_answers else None
    )

    questions.loc[
        questions["question"].str.contains("CosmicDS"), "question_category"
    ] = "intro"
    questions.loc[
        questions["question"].str.contains("Duration \(in seconds\)"),
        "question_category",
    ] = "stat"
    questions.loc[
        questions["both"] & (questions["question_category"].apply(lambda x: x is None)),
        "question_category",
    ] = "science"

    assert (
        ~questions.groupby("question_category").get_group("science")["answer"].isnull()
    ).any(), "Some science questions do not have answers"

    return questions


def add_likert_meta(questions):
    """
    Adds Likert group and negate metadata to the questions DataFrame from likert_questions.json.
    """
    with open("likert_questions.json", "r") as f:
        likert_questions_meta = json.load(f)

    likert_groups = {l["question"]: l["group"] for l in likert_questions_meta}
    likert_negate = {l["question"]: l["score"] == "-1" for l in likert_questions_meta}

    # for all of the likert questions add the "group" as a likert_group_column and the "score" is the likert_negate" bool (True/False for -1/1)
    questions["likert_group"] = questions["question"].apply(
        lambda x: frozenset(likert_groups[x]) if x in likert_groups else frozenset([])
    )

    questions["likert_negate"] = questions["question"].apply(
        lambda x: likert_negate[x] if x in likert_negate else None
    )

    questions.loc[questions.is_likert, "groups"] = questions.loc[
        questions.is_likert, "likert_group"
    ]

    return questions

def row_is_correct(row):
    if row["question_category"] != "science":
        return np.nan
    try:
        return row["answer"].lower() in row[f"response"].lower()
    except Exception as e:
        return 0
    
def merge_questions_into_dataframes(
    df_pre, df_post, questions, id_column, id_columns, pre_columns=[]
):
    """
    Melts pre and post DataFrames to long format and merges with question metadata.
    """
    # melt: turns dataframe into long format. so the columns become rows except for the id_Vars and value_name

    df_pre_merged = df_pre.melt(
        # id_vars=[id_column], var_name="question", value_name="response_pre"
        id_vars=id_columns + pre_columns,
        var_name="question",
        value_name="response",
    )
    df_post_merged = df_post.melt(
        # id_vars=[id_column], var_name="question", value_name="response_post"
        id_vars=id_columns,
        var_name="question",
        value_name="response",
    )
    # merge the melted dataframes with the question metadata
    df_pre_merged = df_pre_merged.merge(
        questions, left_on="question", right_on="question", how="left"
    )
    df_post_merged = df_post_merged.merge(
        questions, left_on="question", right_on="question", how="left"
    )
    # drop any row where the question is not in the metadata
    df_pre_merged.dropna(subset=["question"], inplace=True)
    df_post_merged.dropna(subset=["question"], inplace=True)

    df_pre_merged[id_column] = df_pre_merged[id_column].astype(str).str.lower()
    df_post_merged[id_column] = df_post_merged[id_column].astype(str).str.lower()
    
    df_pre_merged['correct'] = df_pre_merged.apply(row_is_correct, axis=1)
    df_post_merged['correct'] = df_post_merged.apply(row_is_correct, axis=1)
    
    # df_pre_merged[id_column] = pd.Categorical(
    #     df_pre_merged[id_column].astype(str).str.lower(), 
    #     categories=df_pre[id_column].astype(str).str.lower(), ordered=True)
    # df_post_merged[id_column] = pd.Categorical(
    #     df_post_merged[id_column].astype(str).str.lower(), 
    #     categories=df_post[id_column].astype(str).str.lower(), ordered=True)
    
    # df_pre_merged = df_pre_merged.sort_values(by=[id_column])
    # df_post_merged = df_post_merged.sort_values(by=[id_column])


    print(len(df_pre_merged), len(df_post_merged))
    return df_pre_merged, df_post_merged


def get_class_info(df_combined: pd.DataFrame, id_column):
    """
    Merges class and educator info from the database into the combined DataFrame.
    """
    student_ids = list(filter(lambda x: x.isnumeric(), df_combined[id_column].unique()))
    class_info = db.get_students_classes_info(student_ids)
    
    # Helper function to infer values from pre/post inputs
    def infer_common_value(pre_val, post_val):
        pre_val = str(pre_val).strip().lower() if pd.notna(pre_val) else ""
        post_val = str(post_val).strip().lower() if pd.notna(post_val) else ""
        if pre_val == post_val:
            return pre_val
        return post_val or pre_val or ""

    # Handle case when no class_info is available
    if class_info is None or class_info.empty:
        df_combined["class_name"] = df_combined.apply(
            lambda row: infer_common_value(
                row.get("CosmicDS Pre-Survey - Course/Section_pre"),
                row.get("CosmicDS Pre-Survey - Course/Section_post")
            ).title(), axis=1
        )
        df_combined["Educator"] = df_combined.apply(
            lambda row: infer_common_value(
                row.get("CosmicDS Pre-Survey - Instructor's Last Name_pre"),
                row.get("CosmicDS Pre-Survey - Instructor's Last Name_post")
            ).title(), axis=1
        )
        return df_combined
    

    class_info["student_id"] = class_info["student_id"].astype(str).str.lower() 
    
    class_info["class_name"] = class_info["name"].astype(str).str.strip().str.lower()
    class_info["Educator"] = (
        class_info["first_name"].astype(str).str.strip().str.title() + " " +
        class_info["last_name"].astype(str).str.strip().str.title()
    )
    class_info["Educator_Last"] = class_info["last_name"].astype(str).str.strip().str.lower()

    # Select and rename necessary columns
    class_info_subset = class_info[["student_id", "class_name", "id", "Educator", "Educator_Last"]].rename(
        columns={"id": "class_id"}
    )



    # merge the class info into the combined dataframe
    df_combined = df_combined.copy()
    df_combined[id_column] = df_combined[id_column].astype(str).str.lower()
    df_combined = df_combined.merge(
        class_info_subset, left_on=id_column, right_on="student_id", how="left"
    )
    
    # Infer missing class_name and Educator_Last from survey
    inferred_class_names = df_combined.apply(
        lambda row: infer_common_value(
            row.get("CosmicDS Pre-Survey - Course/Section_pre"),
            row.get("CosmicDS Pre-Survey - Course/Section_post")
        ),
        axis=1
    ).str.lower()

    inferred_educators_last = df_combined.apply(
        lambda row: infer_common_value(
            row.get("CosmicDS Pre-Survey - Instructor's Last Name_pre"),
            row.get("CosmicDS Pre-Survey - Instructor's Last Name_post")
        ),
        axis=1
    ).str.lower()
    
    ## Fill in missing class names and educator last names
    df_combined["class_name"] = df_combined["class_name"].combine_first(inferred_class_names).str.title()
    df_combined["Educator_Last"] = df_combined["Educator_Last"].combine_first(inferred_educators_last)

    # Rebuild Educator full name if missing
    df_combined["Educator"] = df_combined["Educator"].fillna(
        df_combined["Educator_Last"].apply(lambda x: str(x).strip().title() if pd.notna(x) else "")
    )

    # Fill in class_id based on same class_name and Educator_Last
    mask_missing_class_id = df_combined["class_id"].isna()
    lookup_group = df_combined[~df_combined["class_id"].isna()].copy()

    # --- Lookup prep ---
    # Create composite key for grouping
    lookup_group = df_combined[~df_combined["class_id"].isna()].copy()
    lookup_group["key"] = (
        lookup_group["class_name"].str.lower().fillna("") + "|" + 
        lookup_group["Educator_Last"].str.lower().fillna("")
    )

    # Drop duplicates, keeping the first valid entry for each key
    lookup_reference = lookup_group.drop_duplicates("key")[["key", "class_id", "Educator"]].set_index("key")

    class_id_lookup = lookup_reference["class_id"].to_dict()
    educator_lookup = lookup_reference["Educator"].to_dict()

    # --- Build fill keys for full df_combined ---
    fill_keys = (
        df_combined["class_name"].str.lower().fillna("") + "|" +
        df_combined["Educator_Last"].str.lower().fillna("")
    )

    # --- Fill missing class_id and Educator where class_id is currently missing ---
    mask_missing_class_id = df_combined["class_id"].isna()

    df_combined.loc[mask_missing_class_id, "class_id"] = fill_keys[mask_missing_class_id].map(class_id_lookup)
    df_combined.loc[mask_missing_class_id, "Educator"] = fill_keys[mask_missing_class_id].map(educator_lookup)
    
    return df_combined

def preview_group(grouped):
    """
    Returns the first group from a pandas GroupBy object for previewing.
    """
    groups = grouped.groups
    first_group = next(iter(groups))
    group = grouped.get_group(first_group)
    return group


def effect_size(pre_vals, post_vals):
    """
    Calculates Cohen's d effect size between pre and post values.
    Cohen's d for Welch test
    The Welch test is a variant of t-test used when the equality of variance can't be assumed. 
    The effect size can be computed by dividing the mean 
    difference between the groups by the “averaged” standard deviation.
    https://www.datanovia.com/en/lessons/t-test-effect-size-using-cohens-d-measure/
    """
    # calculate the effect size
    mean_diff = np.nanmean(post_vals - pre_vals, axis=0)
    pooled_std = np.sqrt(
        (np.nanstd(pre_vals, axis=0) ** 2 + np.nanstd(post_vals, axis=0) ** 2) / 2
    )
    return np.around(mean_diff / pooled_std, 4)


def flatten(l):
    """
    Flattens a list of lists or splits a string by comma.
    """
    if isinstance(l, str):
        return l.split(",")
    if hasattr(l, "__iter__"):
        o = []
        for i, item in enumerate(l):
            if hasattr(item, "__iter__") and not isinstance(item, str):
                for j in item:
                    o.append(j)
            else:
                o.append(item)
        return o
    return [item for sublist in l for item in sublist ]


def q_to_num(qtag):
    """
    Converts a question tag to a float for sorting (e.g., Q1 -> 1.0).
    """
    try:
        return float(qtag[1:].replace("_", ".").upper().replace(".TEXT", ""))
    except ValueError:
        # regex to extract the number from the string
        match = regex.search(r"(\d+)", qtag)
        if match:
            return float(match.group(1))
        else:
            return np.inf


def create_stats_for_pre(group, columns):
    """
    Returns counts and percentages for pre-survey responses in a group.
    """
    # counts = group["response_pre"].explode().value_counts()
    # percentages = group["response_pre"].explode().value_counts(normalize=True)
    # return pd.DataFrame(
    #     {"counts": counts, "percentages": np.around(percentages * 100, 1)}
    # ).rename_axis("response")
    series = []
    for col in columns:
        counts = group[col].explode().value_counts()
        percentages = group[col].explode().value_counts(normalize=True)
        series.append(
            pd.DataFrame(
                {
                    "counts": counts,
                    "percentages": np.around(percentages * 100, 1),
                }
            ).rename_axis(col)
        )
    return series


def create_post_reflection_summary(qoip, id_column):
    """
    Returns a tuple of pd.Series summarizing post-reflection questions (e.g., open text, likert, activity).
    Each Series can be written to a single Excel sheet for summary.
    """
    qs = [
        "14. A STEM professional is a person who uses science, technology, engineering, or mathematics in their everyday work.Think back to the time just before this program began, and select the picture that best describes the overlap of the image you had of yourself and your image of what a STEM professional is.",
        "15. Select the picture that best describes the overlap of the image you currently have of yourself and your image of what a STEM professional is.",
        "16. How strongly do you agree or disagree with the following statements? - I enjoyed participating in the Cosmic Data Story activities",
        "16. How strongly do you agree or disagree with the following statements? - I learned something new from the Cosmic Data Story activities",
        "19. Please tell us something you learned or discovered while completing the Hubble Data Story.",
        "20. Do you have any other questions, comments, or suggestions?",
        "Favorite Activitiy",
        "Which activity was the most difficult or confusing",
    ]
    def mapping(v):
        return{
        'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7
    }.get(v, np.nan)
    qs0 = qoip[qoip["question"] == qs[0]]["response_post"].value_counts().rename({'': 'NA'})
    qs0.sort_index(inplace=True)
    qs0['mean'] = np.nanmean(qoip[qoip["question"] == qs[0]]["response_post"].apply(mapping).to_numpy())
    qs0['sttdev'] = np.nanstd(qoip[qoip["question"] == qs[0]]["response_post"].apply(mapping).to_numpy())
    
    qs1 = qoip[qoip["question"] == qs[1]]["response_post"].value_counts().rename({'': 'NA'})
    qs1.sort_index(inplace=True)
    qs1['mean'] = np.nanmean(qoip[qoip["question"] == qs[1]]["response_post"].apply(mapping).to_numpy())
    qs1['sttdev'] = np.nanstd(qoip[qoip["question"] == qs[1]]["response_post"].apply(mapping).to_numpy())
    
    
    qs2_likert = qoip[qoip["question"] == qs[2]]["response_post"].value_counts() # 
    qs2_likert['mean'] = np.nanmean(qoip[qoip["question"] == qs[2]]["response_post"].apply(pd.to_numeric))
    qs2_likert['sttdev'] = np.nanstd(qoip[qoip["question"] == qs[2]]["response_post"].apply(pd.to_numeric))
    
    qs3_likert = qoip[qoip["question"] == qs[3]]["response_post"].value_counts()
    qs3_likert['mean'] = np.nanmean(qoip[qoip["question"] == qs[3]]["response_post"].apply(pd.to_numeric))
    qs3_likert['sttdev'] = np.nanstd(qoip[qoip["question"] == qs[3]]["response_post"].apply(pd.to_numeric))
    
    qs4_text = qoip[qoip["question"] == qs[4]]["response_post"]
    filtered_qs4_text = qs4_text != ""
    qs5_text = qoip[qoip["question"] == qs[5]]["response_post"]
    filtered_qs5_text = qs5_text != ""
    qs6_text = qoip[qoip["question"] == qs[6]]["response_post"].explode().value_counts()
    qs7_text = qoip[qoip["question"] == qs[7]]["response_post"].explode().value_counts()
    return (
        pd.Series(qs0, name="Select a picture of a STEM Professional"),
        pd.Series(qs1, name="Select picture that overlaps you and a STEM Professional"),
        pd.Series(qs2_likert, name="Enjoyed Participating Likert"),
        pd.Series(qs3_likert, name="Learned Something New Likert"),
        pd.Series(
            qs4_text.values[filtered_qs4_text],
            name="What do you learn or discover",
            index=qoip[qoip["question"] == qs[4]][id_column][filtered_qs4_text],
        ),
        pd.Series(
            qs5_text.values[filtered_qs5_text],
            name="Questions Comments Suggestions",
            index=qoip[qoip["question"] == qs[5]][id_column][filtered_qs5_text],
        ),
        pd.Series(qs6_text ,name="Favorite Activity"),
        pd.Series(qs7_text, name="Difficult of Confusing Activity"),
    )


from scipy.stats import wilcoxon


def wilcoxon_signed_rank_test(pre, post):
    """
    Performs a Wilcoxon signed-rank test on pre and post responses.
    Returns the test statistic and p-value.
    """
    if len(pre) == 0 or len(post) == 0:
        return np.nan, np.nan, np.nan

    try:
        o = wilcoxon(
            (post - pre),
            zero_method="wilcox",
            alternative="greater",
            correction=False,
            method="approx",
        )
        return o.statistic, o.pvalue, o.zstatistic  # type: ignore
    except ValueError as e:
        print(f"Wilcoxon test failed: {e}")
        print(f"Pre: {pre}")
        print(f"Post: {post}")
        return np.nan, np.nan, np.nan


def create_likert_summary(group, force_improved=False, drop_na=False):
    """
    Summarizes Likert question responses, including effect size, means, medians, and counts.
    If force_improved is True, reverses scores for negatively worded questions.
    """
    if force_improved:
        pre_response = group.apply(
            lambda row: (
                5 - row["response_pre"] if row["likert_negate"] else row["response_pre"]
            ),
            axis=1,
        )
        post_response = group.apply(
            lambda row: (
                5 - row["response_post"]
                if row["likert_negate"]
                else row["response_post"]
            ),
            axis=1,
        )
    else:
        pre_response = group["response_pre"]
        post_response = group["response_post"]

    negate = group["likert_negate"].apply(lambda x: -1 if x else 1)
    is_na = pre_response.isna() | post_response.isna()
    pre_nona = pre_response[~is_na]
    post_nona = post_response[~is_na]

    wstat, wpval, wzstat = wilcoxon_signed_rank_test(
        pre_nona.astype(float), post_nona.astype(float)
    )

    summary = pd.Series(
        {
            "tag_pre": group["tag_pre"].iloc[0],
            "effect_size": effect_size(pre_nona, post_nona),
            # "wilcoxon_stat": wilcoxon_stat,
            "wilcoxon_p": wpval,
            "wilcoxon_zstatistic": wzstat,
            "negate": negate.iloc[0],
            "mean(Δ)": np.around((post_nona - pre_nona).mean(), 4),
            "median(Δ)": np.around((post_nona - pre_nona).median(), 4),
            "pooled_std": np.around(
                np.sqrt((np.nanstd(pre_nona) ** 2 + np.nanstd(post_nona) ** 2) / 2), 4
            ),
            "mean(pre)": np.around(pre_nona.mean(), 4),
            "mean(post)": np.around(post_nona.mean(), 4),
            "std(pre)": np.around(pre_nona.std(), 4),
            "std(post)": np.around(post_nona.std(), 4),
            "#_of_pre_responses": len(pre_nona),
            "#_of_post_responses": len(post_nona),
            "median(pre)": np.around(pre_nona.median(), 4),
            "median(post)": np.around(post_nona.median(), 4),
            "min(pre)": pre_nona.min(),
            "min(post)": post_nona.min(),
            "max(pre)": pre_nona.max(),
            "max(post)": post_nona.max(),
            "group": ", ".join(flatten(group["groups"].unique())),
        }
    )

    if not drop_na:
        summary_na = pd.Series(
            {
                "pre_mean_with_no_drop": np.around(pre_response.mean(), 2),
                "post_mean_with_no_drop": np.around(post_response.mean(), 2),
                "pre_count_with_no_drop": len(pre_response),
                "post_count_with_no_drop": len(post_response),
            }
        )
        summary = pd.concat([summary, summary_na])

    return summary


def binomial_var(n, nc=None, raw_count_variance=False):
    """
    Calculates binomial variance for n trials and nc successes.
    """
    if nc is None:
        nc = sum(n)
        n = len(n)
    p = nc / n
    if raw_count_variance:
        return n * p * (1 - p)
    return p * (1 - p) / n


def pooled_variance(variances, sizes=None):
    """
    Calculates pooled variance from a list of variances and sample sizes.
    """
    if sizes is None:
        return np.sum(variances) / len(variances)

    else:
        sizes = np.asarray(sizes)
        variances = np.asarray(variances)
        return np.sum((sizes - 1) * variances) / np.sum(sizes - 1)


def binomial_effect_size(pre, post, raw_count_variance=True):
    """
    Calculates effect size for binary (correct/incorrect) pre/post responses using binomial variance.
    """
    n_pre = len(pre)
    nc_pre = sum(pre)
    n_post = len(post)
    nc_post = sum(post)

    p_pre = nc_pre / n_pre
    p_post = nc_post / n_post

    var_pre = binomial_var(n_pre, nc_pre, raw_count_variance=raw_count_variance)
    var_post = binomial_var(n_post, nc_post, raw_count_variance=raw_count_variance)
    pooled = pooled_variance([var_pre, var_post], [n_pre, n_post])

    if raw_count_variance:
        return (nc_post - nc_pre) / np.sqrt(pooled) if pooled > 0 else float("inf"), np.sqrt(pooled)
    return (p_post - p_pre) / np.sqrt(pooled) if pooled > 0 else float("inf"), np.sqrt(pooled)

def standardized_differance(pre, post):
    """
    Calculates standardized difference between pre and post responses.
    """
    n_pre = len(pre)
    nc_pre = sum(pre)
    n_post = len(post)
    nc_post = sum(post)
    p_pre = nc_pre / n_pre
    p_post = nc_post / n_post
    
    var_pre = binomial_var(n_pre, nc_pre, raw_count_variance=False)
    var_post = binomial_var(n_post, nc_post, raw_count_variance=False)
    var = var_pre + var_post
    
    if var > 0:
        return (p_post - p_pre) / np.sqrt(var)
    else:
        return float("inf")
    
def two_samp_z_for_proportions(pre, post):
    """
    Calculates z-statistic for two-sample proportions test.
    """
    n_pre = len(pre)
    nc_pre = sum(pre)
    n_post = len(post)
    nc_post = sum(post)

    p_pre = nc_pre / n_pre
    p_post = nc_post / n_post

    pooled_p = (nc_pre + nc_post) / (n_pre + n_post)
    z = (p_post - p_pre) / np.sqrt(pooled_p * (1 - pooled_p) * (1 / n_pre + 1 / n_post))
    
    return z

def cohens_h(pre, post):
    """
    Calculates Cohen's h effect size for binary (correct/incorrect) pre/post responses.
    """
    n_pre = len(pre)
    nc_pre = sum(pre)
    n_post = len(post)
    nc_post = sum(post)
    p_pre = nc_pre / n_pre
    p_post = nc_post / n_post
    
    if p_pre == 0:
        p_pre = 0.0001
    if p_post == 0:
        p_post = 0.0001
        
    q_pre = 2 * np.arcsin(np.sqrt(p_pre)) 
    q_post = 2 * np.arcsin(np.sqrt(p_post)) 
    
    return (q_post - q_pre)
    

def are_correct(group, pre=True):
    """
    Returns a boolean Series indicating if each response matches the answer (pre or post).
    """
    if pre:
        return group.apply(
            lambda row: row["answer"].lower() in row["response_pre"].lower(), axis=1
        )
    else:
        return group.apply(
            lambda row: row["answer"].lower() in row["response_post"].lower(), axis=1
        )




def create_content_summary(group):
    """
    Summarizes content (multiple choice) question responses, including effect size, p-values, and summary sentences.
    """
    pre = are_correct(group, pre=True).values
    post = are_correct(group, pre=False).values

    pre = np.array(pre)
    post = np.array(post)
    n_total = len(pre)

    # Count response patterns
    tt = np.sum((pre == True) & (post == True))
    tf = np.sum((pre == True) & (post == False))
    ft = np.sum((pre == False) & (post == True))
    ff = np.sum((pre == False) & (post == False))

    effect_size, pooled_variance = binomial_effect_size(pre, post, raw_count_variance=True) 

    n_discordant = tf + ft
    changed_to_true = ft
    started_false = ft + ff
    confused = tf

    class fake:
        pvalue = 1.0
        statistic = 0.0

    if n_discordant == 0:
        mcnemar = fake
    else:
        mcnemar = binomtest(changed_to_true, n_discordant, p=0.5, alternative="greater")

    if started_false == 0:
        false_to_true = fake()
        false_to_true.pvalue = np.nan
        false_to_true.statistic = np.nan
    else:
        false_to_true = binomtest(
            changed_to_true, started_false, p=0.5, alternative="greater"
        )

    overall = binomtest(np.sum(post), n_total, np.sum(pre) / n_total)

    def direction(val):
        return np.sign(val)

    # --- Human-readable overall summary sentence (pre vs post) ---

    net_change = post.sum() - pre.sum()
    n_total = len(pre)

    if n_discordant == 0:
        summary_sentence = "No students changed their answers."
    else:
        improvement_direction = (
            "improvement"
            if net_change > 0
            else "decline" if net_change < 0 else "no change"
        )

        significance_label = (
            "statistically significant"
            if overall.pvalue < 0.05
            else "not statistically significant"
        )

        summary_sentence = (
            f"There was a {significance_label} {improvement_direction} from pre to post "
            f"({pre.sum()}/{n_total} correct → {post.sum()}/{n_total} correct, "
            f"net change = {net_change:+d}). "
            f"Effect size = {effect_size:.2f}, p = {overall.pvalue:.3f}."
        )

    if n_discordant == 0:
        discordant_summary = "No students changed their answers."
    else:
        improvement_rate = changed_to_true / n_discordant
        confusion_rate = confused / n_discordant
        discordant_significance = (
            "statistically significant improvement among changers"
            if mcnemar.pvalue < 0.05
            else "no statistically significant improvement among changers"
        )
        discordant_summary = (
            f"Among {n_discordant} students who changed, {changed_to_true} improved ({improvement_rate:.1%}) "
            f"and {confused} regressed ({confusion_rate:.1%}) "
            f"({discordant_significance}, McNemar p = {mcnemar.pvalue:.3f})."
        )

    return pd.Series(
        {
            "percent_correct_pre": f"{pre.sum() / n_total:.1%}",
            "percent_correct_post": f"{post.sum() / n_total:.1%}",
            "pre_to_post_change_percent": f"{(post.sum() - pre.sum()) / n_total:.1%}",
            
            "summary": summary_sentence,
            "effect_size": effect_size,
            "sqrt_pooled_variance": pooled_variance,
            "p_value": np.around(overall.pvalue, 6),
            
            "cohens_h": cohens_h(pre, post),
            
            "mcnemar_statistic": mcnemar.statistic,
            "mcnemar_p": mcnemar.pvalue,
            "discordant_summary": discordant_summary,
            "confused_count": tf,
            "confused": tf / (tf + tt) if (tf + tt > 0) else np.nan,
            "confused_p_value": (
                binomtest(confused, tf + tt, p=0.5, alternative="greater").pvalue
                if (tf + tt > 0)
                else np.nan
            ),
            "n_discordant": n_discordant,
            "n_change_to_true": changed_to_true,
            
            "standardized_difference": standardized_differance(pre, post),
            "z_statistic": two_samp_z_for_proportions(pre, post),
            "false_to_true_p_value": np.around(false_to_true.pvalue, 6),
            
            "pre_to_post_change": post.sum() - pre.sum(),
            
            "direction": direction(post.sum() - pre.sum()),
            "tag_pre": group["tag_pre"].iloc[0],
            "tag_post": group["tag_post"].iloc[0],
            "pre_correct": pre.sum(),
            "post_correct": post.sum(),
            "pre_count": group["response_pre"].count(),
            "post_count": group["response_post"].count(),
            "group": ", ".join(flatten(group["groups"].unique())),
        }
    )


def add_answer_column(questions_both):
    """
    Adds columns to a wide-format DataFrame indicating correctness of pre and post responses for each question.
    """

    def check_correct(row, resp_col, ans_col):
        resp = row[resp_col]
        ans = row[ans_col]
        if pd.isnull(resp) or pd.isnull(ans):
            return np.nan
        return int(str(ans).lower() in str(resp).lower())

    # For each question (tag, text), add correctness columns for pre and post
    qb = questions_both.copy()
    for tag in qb.columns.get_level_values(1).unique():
        qtext = qb.columns[qb.columns.get_level_values(1) == tag][0][2]
        pre_col = ("response_pre", tag, qtext)
        post_col = ("response_post", tag, qtext)
        ans_col = ("answer", tag, qtext)
        qb[("correct_pre", tag, qtext)] = qb.apply(
            lambda row: check_correct(row, pre_col, ans_col), axis=1
        )
        qb[("correct_post", tag, qtext)] = qb.apply(
            lambda row: check_correct(row, post_col, ans_col), axis=1
        )

    # Reorder columns so that each correct column is right after its response
    def reorder_columns(df):
        cols = []
        for col in df.columns:
            if col[0] in ["response_pre", "response_post"]:
                cols.append(col)
                correct_col = (
                    "correct_pre" if col[0] == "response_pre" else "correct_post",
                    col[1],
                    col[2],
                )
                if correct_col in df.columns:
                    cols.append(correct_col)
            elif col[0] == "answer":
                cols.append(col)
        return df[cols]

    return reorder_columns(qb)
