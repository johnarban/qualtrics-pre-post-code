# Generated by Copilot
from importlib import reload

import csv
import json
import regex
from io import StringIO


import numpy as np
import pandas as pd


from qualtrics_keys import *
import qualtrics_api as qapi
import nlp_summary

import get_class_from_db as db
from scipy.stats import binomtest

reload(db)
reload(qapi)
reload(nlp_summary)

pd.set_option("display.max_colwidth", None)


def clean_up_description(description: str):
    """
    Cleans up survey question descriptions by removing newlines, non-ascii characters, and extra spaces.
    """
    # remove new line characters
    description = description.replace("\n", "")
    # remove non-ascii characters
    description = description.encode("ascii", "ignore").decode("ascii")
    # remove runs of spaces
    description = regex.sub(r"\s+", " ", description)
    # remove leading and trailing whitespace
    description = description.strip()
    return description


def read_in_qualtrics(f: StringIO):
    """
    Reads a Qualtrics CSV file and returns header, description, formatspec, and data as numpy array.
    """
    reader = csv.reader(f, delimiter=",", quotechar='"')
    header = next(reader)
    description = next(reader)
    description = [clean_up_description(d) for d in description]
    formatspec = next(reader)
    data = [row for row in reader]
    return header, description, formatspec, np.asarray(data)


columns_to_drop = [
    "Status",
    "StartDate",
    "EndDate",
    "Finished",
    "IPAddress",
    "Progress",
    "RecordedDate",
    "ResponseId",
    "RecipientLastName",
    "RecipientFirstName",
    "RecipientEmail",
    "ExternalReference",
    "LocationLatitude",
    "LocationLongitude",
    "DistributionChannel",
    "UserLanguage",
    "Intro information_2",
    "Intro information_3",
]


def load_data(from_api=True, survey_id="", filename=None):
    """
    Loads survey data from API or local CSV file. Writes API data to file if fetched.
    """
    if filename is None:
        filename = survey_id + "_response.csv"

    if from_api:
        pre_response = qapi.get_survey(
            token, data_center, survey_id, format="csv", numeric=False
        )
        if pre_response is not None:
            with open(filename, "w") as f:
                f.write(pre_response)
        return pre_response

    if not from_api:
        with open(filename, "r") as f:
            return f.read()


def load_pre_data(from_api=True, filename="2025_pre_response.csv"):
    """
    Loads pre-survey data using load_data().
    """
    return load_data(from_api, survey_id_2025_pre, filename)


def load_post_data(from_api=True, filename="2025_post_response.csv"):
    """
    Loads post-survey data using load_data().
    """
    return load_data(from_api, survey_id_2025_post, filename)


def parse_response(response):
    """
    Parses a Qualtrics CSV response string into header, description, and data.
    """
    with StringIO(response) as f:
        tuple = read_in_qualtrics(f)
        header, description, _, data = tuple

    assert len(header) == len(
        description
    ), f"Header and description lengths do not match: {len(header)} != {len(description)}"
    assert len(header) == len(
        data[0]
    ), f"Header and data lengths do not match: {len(header)} != {len(data[0])}"

    return header, description, data


def column_cleanup(
    header_pre,
    description_pre,
    data_pre,
    header_post,
    description_post,
    data_post,
    drop_columns=[],
):
    """
    Drops specified columns from pre and post survey data and aligns question descriptions.
    """
    for i in range(len(description_post)):
        if "Post-Survey" in description_post[i]:
            description_post[i] = description_post[i].replace(
                "Post-Survey", "Pre-Survey"
            )

    for col in drop_columns:
        if col in description_post:
            post_idx = description_post.index(col)
            print(f"Dropping column {col} from post survey")
            header_post.pop(post_idx)
            description_post.pop(post_idx)
            data_post = np.delete(data_post, post_idx, axis=1)
        if col in description_pre:
            pre_idx = description_pre.index(col)
            print(f"Dropping column {col} from pre survey")
            header_pre.pop(pre_idx)
            description_pre.pop(pre_idx)
            data_pre = np.delete(data_pre, pre_idx, axis=1)


def get_pre_header_for_post_question(
    question, header_pre, description_pre, header_post, description_post, verbose=False
):
    """
    Ensures that matching questions in pre and post have the same header tag.
    """
    post_idx = description_post.index(question)
    post_header = header_post[post_idx]

    try:
        pre_idx = description_pre.index(question)
    except ValueError:
        if not verbose:
            return
        print(f"Question {question} not found in pre survey")
        return

    pre_header = header_pre[pre_idx]
    if (description_post[post_idx] == description_pre[pre_idx]) and (
        header_post[post_idx] == header_pre[pre_idx]
    ):
        if not verbose:
            return
        print(f"No need to change header {post_header} for question")
        return

    if pre_header not in header_post:
        # print(f"Changing header {post_header} to {pre_header}")
        header_post[post_idx] = pre_header
    else:
        print(f"Header {pre_header} already in post survey")
        pre_idx = header_post.index(pre_header)
        post_idx = header_post.index(pre_header)
        pre_q = description_pre[pre_idx]
        post_q = description_post[post_idx]
        print(f"Pre: {pre_header} Question: {pre_q}")
        print(f"Post: {pre_header} Question: {post_q}")


def create_initial_dataframes(
    header_pre,
    description_pre,
    data_pre,
    header_post,
    description_post,
    data_post,
    id_column,
):
    """
    Creates pandas DataFrames for pre and post survey data, indexed by id_column. Drops duplicate IDs in pre.
    """
    df_pre = pd.DataFrame(data_pre, columns=description_pre)
    df_post = pd.DataFrame(data_post, columns=description_post)

    if not df_pre[id_column].is_unique:
        duplicates = df_pre[df_pre.duplicated(subset=[id_column], keep=False)][
            id_column
        ].unique()
        print("Dropping duplicate ids:")
        for dup in duplicates:
            print(f"\t{dup}")
        df_pre.drop_duplicates(subset=[id_column], inplace=True)

    return df_pre, df_post


def create_question_dataframe(
    df_pre, df_post, header_pre, description_pre, header_post, description_post
):
    """
    Builds a DataFrame listing all questions, their presence in pre/post, and their tags.
    """
    q = [c for c in df_pre.columns]
    _questions = q + [c for c in df_post.columns if c not in q]

    questions_in_common = [
        c in df_post.columns and c in df_pre.columns for c in _questions
    ]
    questions_in_pre = [c in df_pre.columns for c in _questions]
    questions_in_post = [c in df_post.columns for c in _questions]
    tag_pre = [
        header_pre[description_pre.index(q)] if q in description_pre else None
        for q in _questions
    ]
    tag_post = [
        header_post[description_post.index(q)] if q in description_post else None
        for q in _questions
    ]
    print(f"Questions in common: {sum(questions_in_common)}")
    print(f"Questions in pre: {sum(questions_in_pre)}")
    print(f"Questions in post: {sum(questions_in_post)}")

    return pd.DataFrame(
        {
            "question": _questions,
            "both": questions_in_common,
            "in_pre": questions_in_pre,
            "in_post": questions_in_post,
            "tag_pre": tag_pre,
            "tag_post": tag_post,
            "question_category": [None] * len(_questions),
        },
    )


def is_likert_answer(x):
    """
    Returns True if the answer string is a Likert scale response.
    """
    if isinstance(x, str):
        return any(
            [
                "strongly disagree" in x.lower(),
                "strongly agree" in x.lower(),
            ]
        )
    return False


def is_likert_column(x: pd.Series):
    """
    Returns True if any value in the Series is a Likert answer.
    """
    return x.apply(is_likert_answer).any()


def convert_likert_to_numeric(x):
    """
    Converts a Likert response to a numeric value (if possible).
    """
    if len(x) == 0:
        return np.nan
    return int(x[0])


def add_likert_columns(questions, df_pre, df_post):
    """
    Adds a boolean column to questions DataFrame indicating Likert questions.
    """
    likert_columns = set(df_pre.columns[df_pre.apply(is_likert_column)]).union(
        set(df_post.columns[df_post.apply(is_likert_column)])
    )
    questions["is_likert"] = questions["question"].apply(lambda x: x in likert_columns)
    questions.loc[questions["is_likert"], "question_category"] = "likert"


# We need to reconstruct the parent demographic question. The API returns this split accross


def process_parent_questions(questions, df_pre, df_post, sep=2):
    """
    Combines parent demographic questions split across multiple columns into single columns.
    """
    # identify the parent education and gender questions.
    parent1_edu = questions.loc[
        questions["question"].str.contains("Parent/Guardian 1")
    ]["question"][sep:]
    parent1_gen = questions.loc[
        questions["question"].str.contains("Parent/Guardian 1")
    ]["question"][:sep]

    parent2_edu = questions.loc[
        questions["question"].str.contains("Parent/Guardian 2")
    ]["question"][sep:]
    parent2_gen = questions.loc[
        questions["question"].str.contains("Parent/Guardian 2")
    ]["question"][:sep]

    education_map = {
        "": 0,
        "Bachelor’s Degree": 1,
        "Less than High School": 2,
        "High School Diploma/GED": 3,
        "Some College/ Associate Degree": 4,
        "Master’s Degree or higher": 5,
    }
    # get the highest education level of the parent
    df_pre["Parent 1 Education"] = df_pre[parent1_edu].apply(
        lambda x: max(x, key=lambda i: education_map.get(i, 0)) or "(empty)", axis=1
    )
    df_pre["Parent 2 Education"] = df_pre[parent2_edu].apply(
        lambda x: max(x, key=lambda i: education_map.get(i, 0)) or "(empty)", axis=1
    )
    df_pre["Parent 1 Gender"] = df_pre[parent1_gen].apply(
        lambda x: max(x) or "(empty)", axis=1
    )

    df_pre["Parent 2 Gender"] = df_pre[parent2_gen].apply(
        lambda x: max(x) or "(empty)", axis=1
    )

    # add these to the questions
    new_questions = [
        {
            "question": "Parent 1 Education",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Parent/Guardian 1")
            ]["tag_pre"][sep:].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
        {
            "question": "Parent 1 Gender",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Parent/Guardian 1")
            ]["tag_pre"][:sep].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
        {
            "question": "Parent 2 Education",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Parent/Guardian 2")
            ]["tag_pre"][sep:].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
        {
            "question": "Parent 2 Gender",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Parent/Guardian 2")
            ]["tag_pre"][:sep].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]
    # remove the old questions
    questions = questions.loc[~questions["question"].isin(parent1_edu)]
    questions = questions.loc[~questions["question"].isin(parent2_edu)]
    questions = questions.loc[~questions["question"].isin(parent1_gen)]
    questions = questions.loc[~questions["question"].isin(parent2_gen)]
    # add the new questions
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return (
        questions,
        parent1_edu.tolist()
        + parent2_edu.tolist()
        + parent1_gen.tolist()
        + parent2_gen.tolist(),
    )


def process_race_questions(questions, df_pre, df_post):
    """
    Combines race and Hispanic origin questions into a single 'Student Race' column.
    """
    # combine the race questions
    race_questions = questions.loc[
        questions["question"].str.contains("With which race do you identify")
        | questions["question"].str.contains("Hispanic")
    ]["question"]

    # what magic is frozenset(filter(bool,v)) doing? it is removing all the empty strings from the list
    df_pre["Student Race"] = df_pre[race_questions].apply(
        lambda v: frozenset(filter(bool, v)), axis=1
    )

    # some of these will have a Yes, No for the hispanic question, we want to replace yes with Hispanic and no with nothing
    def translate(x):
        if x == "Yes":
            return "Hispanic"
        elif x == "No":
            return "Not Hispanic"
        else:
            return x

    df_pre["Student Race"] = (
        df_pre["Student Race"]
        .apply(lambda x: frozenset([translate(i).strip() for i in x]))
        .apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))
    )

    new_questions = [
        {
            "question": "Student Race",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("With which race do you identify")
            ]["tag_pre"].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(race_questions)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, race_questions


# 19. With which gender do you identify? - Prefer to Self Describe - Text', '19. With which gender do you identify? - Selected Choice'
def process_gender_questions(questions, df_pre, df_post):
    """
    Combines gender identity questions into a single 'Student Gender' column.
    """
    # combine
    gender_questions = questions.loc[
        questions["question"].str.contains("With which gender do you identify")
    ]["question"]

    df_pre["Student Gender"] = (
        df_pre[gender_questions]
        .apply(lambda v: frozenset(filter(bool, v)), axis=1)
        .apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))
    )

    new_questions = [
        {
            "question": "Student Gender",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("With which gender do you identify")
            ]["tag_pre"].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(gender_questions)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, gender_questions


def process_school_questions(questions, df_pre, df_post):  # school questions
    """
    Combines school level questions into a single 'Student School Level' column.
    """
    # combine the race questions
    school_questions = questions.loc[questions["question"].str.contains("Are you in")][
        "question"
    ]

    # what magic is frozenset(filter(bool,v)) doing? it is removing all the empty strings from the list
    df_pre["Student School Level"] = (
        df_pre[school_questions]
        .apply(lambda v: frozenset(filter(bool, v)), axis=1)
        .apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))
    )

    new_questions = [
        {
            "question": "Student School Level",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[questions["question"].str.contains("Are you in")][
                "tag_pre"
            ].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(school_questions)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, school_questions


def process_activity_questions(questions, df_pre, df_post):
    """
    Combines activity reflection questions into a single 'Favorite Activity' column.
    """
    # school questions
    # combine the race questions
    # escape cuz it treats it like a regex
    activity_question = questions.loc[
        questions["question"].str.contains("Which activity \(or activities\) did you")
    ]["question"]

    # what magic is frozenset(filter(bool,v)) doing? it is removing all the empty strings from the list
    df_post["Favorite Activitiy"] = (
        df_post[activity_question]
        .apply(lambda v: frozenset(filter(bool, v)), axis=1)
        .apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))
    )

    new_questions = [
        {
            "question": "Favorite Activitiy",
            "both": False,
            "in_pre": False,
            "in_post": True,
            "tag_pre": questions.loc[
                questions["question"].str.contains(
                    "Which activity \(or activities\) did you"
                )
            ]["tag_post"].values[0],
            "tag_post": None,
            "question_category": "reflection",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(activity_question)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, activity_question


def process_confused_questions(questions, df_pre, df_post):
    """
    Combines 'most difficult or confusing activity' questions into a single column.
    """
    # school questions
    # combine the race questions
    # escape cuz it treats it like a regex
    _question = questions.loc[
        questions["question"].str.contains(
            "Which activity was the most difficult or confusing"
        )
    ]["question"]

    # what magic is frozenset(filter(bool,v)) doing? it is removing all the empty strings from the list
    df_post["Which activity was the most difficult or confusing"] = (
        df_post[_question]
        .apply(lambda v: frozenset(filter(bool, v)), axis=1)
        .apply(lambda x: x if len(x) > 0 else frozenset(["(empty)"]))
    )

    new_questions = [
        {
            "question": "Which activity was the most difficult or confusing",
            "both": False,
            "in_pre": False,
            "in_post": True,
            "tag_pre": questions.loc[
                questions["question"].str.contains(
                    "Which activity was the most difficult or confusing"
                )
            ]["tag_post"].values[0],
            "tag_post": None,
            "question_category": "reflection",
            "is_likert": False,
        },
    ]

    questions = questions.loc[~questions["question"].isin(_question)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)

    return questions, _question


def process_esl_questions(questions, df_pre, df_post):
    """
    Combines ESL (English as a Second Language) questions into a single column.
    """
    # school questions
    # combine
    esl_questions = questions.loc[
        questions["question"].str.contains("Is English the primary spoken")
    ]["question"]

    def isESL(val):
        if val == "No":
            return "ESL"
        elif val == "Yes":
            return "Not ESL"
        elif val == "":
            return "(empty)"
        return val

    df_pre["Student ESL"] = df_pre[esl_questions].apply(
        lambda x: next((isESL(i) for i in x), ("(empty)")), axis=1
    )

    new_questions = [
        {
            "question": "Student ESL",
            "both": False,
            "in_pre": True,
            "in_post": False,
            "tag_pre": questions.loc[
                questions["question"].str.contains("Is English the primary spoken")
            ]["tag_pre"].values[0],
            "tag_post": None,
            "question_category": "demographics",
            "is_likert": False,
        },
    ]
    questions = questions.loc[~questions["question"].isin(esl_questions)]
    questions = pd.concat([questions, pd.DataFrame(new_questions)], ignore_index=True)
    return questions, esl_questions


def process_likert_columns(df, questions):
    """
    Converts Likert columns in the DataFrame to numeric values using convert_likert_to_numeric().
    """
    # convert the likert columns to numeric
    for col in df.columns:
        if col in questions["question"].values:
            if questions.loc[questions["question"] == col]["is_likert"].values[0]:
                df[col] = df[col].apply(lambda x: convert_likert_to_numeric(x))
    return df


def add_content_question_meta(questions, description_pre, description_post, quiet=True):
    """
    Adds metadata (answers, groups, summaries) to the questions DataFrame from qa_pairs.json.
    """
    """All changeds are don in place"""
    with open("qa_pairs.json", "r") as f:
        qa_pairs = json.load(f)

    if not quiet:
        print(f"There are {len(qa_pairs)} QA pairs")

    questions_and_answers = {q["question"]: q["answer"] for q in qa_pairs}

    # make sure every q & a is in description_pre_tex and description_post
    for w in qa_pairs:
        question = w["question"]
        if (question not in description_pre) and (question not in description_post):
            if not quiet:
                print(f"Removing question {question} from qa_pairs")
            del questions_and_answers[question]

    # Build a mapping from question to groups
    question_to_groups = {q["question"]: q.get("group", []) for q in qa_pairs}
    questions["groups"] = (
        questions["question"]
        .map(question_to_groups)
        .apply(lambda x: frozenset(x) if str(x) != "nan" else frozenset([]))
    )

    # in questions add an empty column for the answer and the short version of the question
    questions["answer"] = None
    questions["short_question"] = None

    # for questions where is_likert is true, set the short question to the nlp_summary.create_summary
    questions["short_question"] = questions["question"].apply(
        lambda x: nlp_summary.create_summary(x.split("? -", maxsplit=1)[-1].strip(), 6)
    )

    questions["answer"] = questions["question"].apply(
        lambda x: questions_and_answers[x] if x in questions_and_answers else None
    )

    questions.loc[
        questions["question"].str.contains("CosmicDS"), "question_category"
    ] = "intro"
    questions.loc[
        questions["question"].str.contains("Duration \(in seconds\)"),
        "question_category",
    ] = "stat"
    questions.loc[
        questions["both"] & (questions["question_category"].apply(lambda x: x is None)),
        "question_category",
    ] = "science"

    assert (
        ~questions.groupby("question_category").get_group("science")["answer"].isnull()
    ).any(), "Some science questions do not have answers"

    return questions


def add_likert_meta(questions):
    """
    Adds Likert group and negate metadata to the questions DataFrame from likert_questions.json.
    """
    with open("likert_questions.json", "r") as f:
        likert_questions_meta = json.load(f)

    likert_groups = {l["question"]: l["group"] for l in likert_questions_meta}
    likert_negate = {l["question"]: l["score"] == "-1" for l in likert_questions_meta}

    # for all of the likert questions add the "group" as a likert_group_column and the "score" is the likert_negate" bool (True/False for -1/1)
    questions["likert_group"] = questions["question"].apply(
        lambda x: frozenset(likert_groups[x]) if x in likert_groups else frozenset([])
    )

    questions["likert_negate"] = questions["question"].apply(
        lambda x: likert_negate[x] if x in likert_negate else None
    )

    questions.loc[questions.is_likert, "groups"] = questions.loc[
        questions.is_likert, "likert_group"
    ]

    return questions


def merge_questions_into_dataframes(
    df_pre, df_post, questions, id_column, id_columns, pre_columns=[]
):
    """
    Melts pre and post DataFrames to long format and merges with question metadata.
    """
    # melt: turns dataframe into long format. so the columns become rows except for the id_Vars and value_name

    df_pre_merged = df_pre.melt(
        # id_vars=[id_column], var_name="question", value_name="response_pre"
        id_vars=id_columns + pre_columns,
        var_name="question",
        value_name="response_pre",
    )
    df_post_merged = df_post.melt(
        # id_vars=[id_column], var_name="question", value_name="response_post"
        id_vars=id_columns,
        var_name="question",
        value_name="response_post",
    )
    # merge the melted dataframes with the question metadata
    df_pre_merged = df_pre_merged.merge(
        questions, left_on="question", right_on="question", how="left"
    )
    df_post_merged = df_post_merged.merge(
        questions, left_on="question", right_on="question", how="left"
    )
    # drop any row where the question is not in the metadata
    df_pre_merged.dropna(subset=["question"], inplace=True)
    df_post_merged.dropna(subset=["question"], inplace=True)

    df_pre_merged[id_column] = df_pre_merged[id_column].astype(str).str.lower()
    df_post_merged[id_column] = df_post_merged[id_column].astype(str).str.lower()

    print(len(df_pre_merged), len(df_post_merged))
    return df_pre_merged, df_post_merged


def get_class_info(df_combined, id_column):
    """
    Merges class and educator info from the database into the combined DataFrame.
    """
    class_info = db.get_students_classes_info(
        frozenset(filter(lambda x: x.isnumeric(), df_combined[id_column].unique()))
    )

    if class_info is None:
        df_combined["class_name"] = df_combined[
            "CosmicDS Pre-Survey - Course/Section_pre"
        ]
        df_combined["Educator"] = df_combined[
            "CosmicDS Pre-Survey - Instructor's Last Name_post"
        ]
        return df_combined

    class_info["student_id"] = class_info["student_id"].astype(str).str.lower()

    # rename columns
    class_colnames = {
        "name": "class_name",
        "id": "class_id",
    }

    class_info["Educator"] = class_info["first_name"] + " " + class_info["last_name"]
    class_info.rename(columns=class_colnames, inplace=True)

    class_info = class_info[["student_id", "class_name", "class_id", "Educator"]]

    # merge the class info into the combined dataframe
    return df_combined.merge(
        class_info, left_on=id_column, right_on="student_id", how="left"
    )


def preview_group(grouped):
    """
    Returns the first group from a pandas GroupBy object for previewing.
    """
    groups = grouped.groups
    first_group = next(iter(groups))
    group = grouped.get_group(first_group)
    return group


def effect_size(pre_vals, post_vals):
    """
    Calculates Cohen's d effect size between pre and post values.
    """
    # calculate the effect size
    mean_diff = np.nanmean(post_vals - pre_vals, axis=0)
    pooled_std = np.sqrt(
        (np.nanstd(pre_vals, axis=0) ** 2 + np.nanstd(post_vals, axis=0) ** 2) / 2
    )
    return np.around(mean_diff / pooled_std, 4)


def flatten(l):
    """
    Flattens a list of lists or splits a string by comma.
    """
    if isinstance(l, str):
        return l.split(",")
    return [item for sublist in l for item in sublist]


def q_to_num(qtag):
    """
    Converts a question tag to a float for sorting (e.g., Q1 -> 1.0).
    """
    try:
        return float(qtag[1:].replace("_", ".").upper().replace(".TEXT", ""))
    except ValueError:
        # regex to extract the number from the string
        match = regex.search(r"(\d+)", qtag)
        if match:
            return float(match.group(1))
        else:
            return np.inf


def create_stats_for_pre(group):
    """
    Returns counts and percentages for pre-survey responses in a group.
    """
    counts = group["response_pre"].explode().value_counts()
    percentages = group["response_pre"].explode().value_counts(normalize=True)
    return pd.DataFrame(
        {"counts": counts, "percentages": np.around(percentages * 100, 1)}
    ).rename_axis("response")


def create_post_reflection_summary(qoip):
    """
    Returns a tuple of pd.Series summarizing post-reflection questions (e.g., open text, likert, activity).
    Each Series can be written to a single Excel sheet for summary.
    """
    qs = [
        "14. A STEM professional is a person who uses science, technology, engineering, or mathematics in their everyday work.Think back to the time just before this program began, and select the picture that best describes the overlap of the image you had of yourself and your image of what a STEM professional is.",
        "15. Select the picture that best describes the overlap of the image you currently have of yourself and your image of what a STEM professional is.",
        "16. How strongly do you agree or disagree with the following statements? - I enjoyed participating in the Cosmic Data Story activities",
        "16. How strongly do you agree or disagree with the following statements? - I learned something new from the Cosmic Data Story activities",
        "19. Please tell us something you learned or discovered while completing the Hubble Data Story.",
        "20. Do you have any other questions, comments, or suggestions?",
        "Favorite Activitiy",
        "Which activity was the most difficult or confusing",
    ]
    qs0 = qoip[qoip["question"] == qs[0]]["response_post"].value_counts()
    qs1 = qoip[qoip["question"] == qs[1]]["response_post"].value_counts()
    qs2_likert = qoip[qoip["question"] == qs[2]]["response_post"]
    qs3_likert = qoip[qoip["question"] == qs[3]]["response_post"]
    qs4_text = qoip[qoip["question"] == qs[4]]["response_post"]
    qs5_text = qoip[qoip["question"] == qs[5]]["response_post"]
    qs6_text = qoip[qoip["question"] == qs[6]]["response_post"].explode().value_counts()
    qs7_text = qoip[qoip["question"] == qs[7]]["response_post"].explode().value_counts()
    return (
        pd.Series(qs0, name="Select a picture of a STEM Professional"),
        pd.Series(qs1, name="Select picture that overlaps you and a STEM Professional"),
        pd.Series(qs2_likert.value_counts(), name="Enjoyed Participating Likert"),
        pd.Series(qs3_likert.value_counts(), name="Learned Something New Likert"),
        pd.Series(
            list(filter(lambda x: x != "", qs4_text.values)),
            name="What do you learn or discover",
        ),
        pd.Series(
            list(filter(lambda x: x != "", qs5_text.values)),
            name="Questions Comments Suggestions",
        ),
        pd.Series(qs6_text, name="Favorite Activity"),
        pd.Series(qs7_text, name="Difficult of Confusing Activity"),
    )


from scipy.stats import wilcoxon


def wilcoxon_signed_rank_test(pre, post):
    """
    Performs a Wilcoxon signed-rank test on pre and post responses.
    Returns the test statistic and p-value.
    """
    if len(pre) == 0 or len(post) == 0:
        return np.nan, np.nan, np.nan

    try:
        o = wilcoxon(
            (post - pre),
            zero_method="wilcox",
            alternative="greater",
            correction=False,
            method="approx",
        )
        return o.statistic, o.pvalue, o.zstatistic  # type: ignore
    except ValueError as e:
        print(f"Wilcoxon test failed: {e}")
        print(f"Pre: {pre}")
        print(f"Post: {post}")
        return np.nan, np.nan, np.nan


def create_likert_summary(group, force_improved=False, drop_na=False):
    """
    Summarizes Likert question responses, including effect size, means, medians, and counts.
    If force_improved is True, reverses scores for negatively worded questions.
    """
    if force_improved:
        pre_response = group.apply(
            lambda row: (
                5 - row["response_pre"] if row["likert_negate"] else row["response_pre"]
            ),
            axis=1,
        )
        post_response = group.apply(
            lambda row: (
                5 - row["response_post"]
                if row["likert_negate"]
                else row["response_post"]
            ),
            axis=1,
        )
    else:
        pre_response = group["response_pre"]
        post_response = group["response_post"]

    negate = group["likert_negate"].apply(lambda x: -1 if x else 1)
    is_na = pre_response.isna() | post_response.isna()
    pre_nona = pre_response[~is_na]
    post_nona = post_response[~is_na]

    wstat, wpval, wzstat = wilcoxon_signed_rank_test(
        pre_nona.astype(float), post_nona.astype(float)
    )

    summary = pd.Series(
        {
            "tag_pre": group["tag_pre"].iloc[0],
            "effect_size": effect_size(pre_nona, post_nona),
            # "wilcoxon_stat": wilcoxon_stat,
            "wilcoxon_p": wpval,
            "wilcoxon_zstatistic": wzstat,
            "negate": negate.iloc[0],
            "mean(Δ)": np.around((post_nona - pre_nona).mean(), 4),
            "median(Δ)": np.around((post_nona - pre_nona).median(), 4),
            "pooled_std": np.around(
                np.sqrt((np.nanstd(pre_nona) ** 2 + np.nanstd(post_nona) ** 2) / 2), 4
            ),
            "mean(pre)": np.around(pre_nona.mean(), 4),
            "mean(post)": np.around(post_nona.mean(), 4),
            "std(pre)": np.around(pre_nona.std(), 4),
            "std(post)": np.around(post_nona.std(), 4),
            "#_of_pre_responses": len(pre_nona),
            "#_of_post_responses": len(post_nona),
            "median(pre)": np.around(pre_nona.median(), 4),
            "median(post)": np.around(post_nona.median(), 4),
            "min(pre)": pre_nona.min(),
            "min(post)": post_nona.min(),
            "max(pre)": pre_nona.max(),
            "max(post)": post_nona.max(),
            "group": ", ".join(flatten(group["groups"].unique())),
        }
    )

    if not drop_na:
        summary_na = pd.Series(
            {
                "pre_mean_with_no_drop": np.around(pre_response.mean(), 2),
                "post_mean_with_no_drop": np.around(post_response.mean(), 2),
                "pre_count_with_no_drop": len(pre_response),
                "post_count_with_no_drop": len(post_response),
            }
        )
        summary = pd.concat([summary, summary_na])

    return summary


def binomial_var(n, nc=None, raw_count_variance=False):
    """
    Calculates binomial variance for n trials and nc successes.
    """
    if nc is None:
        nc = sum(n)
        n = len(n)
    p = nc / n
    if raw_count_variance:
        return n * p * (1 - p)
    return p * (1 - p) / n


def pooled_variance(variances, sizes=None):
    """
    Calculates pooled variance from a list of variances and sample sizes.
    """
    if sizes is None:
        return np.sum(variances) / len(variances)

    else:
        sizes = np.asarray(sizes)
        variances = np.asarray(variances)
        return np.sum((sizes - 1) * variances) / np.sum(sizes - 1)


def binomial_effect_size(pre, post, raw_count_variance=True):
    """
    Calculates effect size for binary (correct/incorrect) pre/post responses using binomial variance.
    """
    n_pre = len(pre)
    nc_pre = sum(pre)
    n_post = len(post)
    nc_post = sum(post)

    p_pre = nc_pre / n_pre
    p_post = nc_post / n_post

    var_pre = binomial_var(n_pre, nc_pre, raw_count_variance=raw_count_variance)
    var_post = binomial_var(n_post, nc_post, raw_count_variance=raw_count_variance)
    pooled = pooled_variance([var_pre, var_post], [n_pre, n_post])

    if raw_count_variance:
        return (p_post - p_pre) / np.sqrt(pooled) if pooled > 0 else float("inf")
    return (nc_post - nc_pre) / np.sqrt(pooled) if pooled > 0 else float("inf")


def are_correct(group, pre=True):
    """
    Returns a boolean Series indicating if each response matches the answer (pre or post).
    """
    if pre:
        return group.apply(
            lambda row: row["answer"].lower() in row["response_pre"].lower(), axis=1
        )
    else:
        return group.apply(
            lambda row: row["answer"].lower() in row["response_post"].lower(), axis=1
        )


def create_content_summary(group):
    """
    Summarizes content (multiple choice) question responses, including effect size, p-values, and summary sentences.
    """
    pre = are_correct(group, pre=True).values
    post = are_correct(group, pre=False).values

    pre = np.array(pre)
    post = np.array(post)
    n_total = len(pre)

    # Count response patterns
    tt = np.sum((pre == True) & (post == True))
    tf = np.sum((pre == True) & (post == False))
    ft = np.sum((pre == False) & (post == True))
    ff = np.sum((pre == False) & (post == False))

    effect_size = binomial_effect_size(pre, post)

    n_discordant = tf + ft
    changed_to_true = ft
    started_false = ft + ff
    confused = tf

    class fake:
        pvalue = 1.0
        statistic = 0.0

    if n_discordant == 0:
        mcnemar = fake
    else:
        mcnemar = binomtest(changed_to_true, n_discordant, p=0.5, alternative="greater")

    if started_false == 0:
        false_to_true = fake()
        false_to_true.pvalue = np.nan
        false_to_true.statistic = np.nan
    else:
        false_to_true = binomtest(
            changed_to_true, started_false, p=0.5, alternative="greater"
        )

    overall = binomtest(np.sum(post), n_total, np.sum(pre) / n_total)

    def direction(val):
        return np.sign(val)

    # --- Human-readable overall summary sentence (pre vs post) ---

    net_change = post.sum() - pre.sum()
    n_total = len(pre)

    if n_discordant == 0:
        summary_sentence = "No students changed their answers."
    else:
        improvement_direction = (
            "improvement"
            if net_change > 0
            else "decline" if net_change < 0 else "no change"
        )

        significance_label = (
            "statistically significant"
            if overall.pvalue < 0.05
            else "not statistically significant"
        )

        summary_sentence = (
            f"There was a {significance_label} {improvement_direction} from pre to post "
            f"({pre.sum()}/{n_total} correct → {post.sum()}/{n_total} correct, "
            f"net change = {net_change:+d}). "
            f"Effect size = {effect_size:.2f}, p = {overall.pvalue:.3f}."
        )

    if n_discordant == 0:
        discordant_summary = "No students changed their answers."
    else:
        improvement_rate = changed_to_true / n_discordant
        confusion_rate = confused / n_discordant
        discordant_significance = (
            "statistically significant improvement among changers"
            if mcnemar.pvalue < 0.05
            else "no statistically significant improvement among changers"
        )
        discordant_summary = (
            f"Among {n_discordant} students who changed, {changed_to_true} improved ({improvement_rate:.1%}) "
            f"and {confused} regressed ({confusion_rate:.1%}) "
            f"({discordant_significance}, McNemar p = {mcnemar.pvalue:.3f})."
        )

    return pd.Series(
        {
            "percent_correct_pre": f"{pre.sum() / n_total:.1%}",
            "percent_correct_post": f"{post.sum() / n_total:.1%}",
            "summary": summary_sentence,
            "effect_size": effect_size,
            "pre_to_post_change": post.sum() - pre.sum(),
            "p_value": np.around(overall.pvalue, 6),
            "direction": direction(post.sum() - pre.sum()),
            "confused_count": tf,
            "confused": tf / (tf + tt) if (tf + tt > 0) else np.nan,
            "confused_p_value": (
                binomtest(confused, tf + tt, p=0.5, alternative="greater").pvalue
                if (tf + tt > 0)
                else np.nan
            ),
            "n_discordant": n_discordant,
            "n_change_to_true": changed_to_true,
            "mcnemar_p": mcnemar.pvalue,
            "discordant_summary": discordant_summary,
            "tag_pre": group["tag_pre"].iloc[0],
            "tag_post": group["tag_post"].iloc[0],
            "pre_correct": pre.sum(),
            "post_correct": post.sum(),
            "pre_count": group["response_pre"].count(),
            "post_count": group["response_post"].count(),
            "group": ", ".join(flatten(group["groups"].unique())),
        }
    )


def add_answer_column(questions_both):
    """
    Adds columns to a wide-format DataFrame indicating correctness of pre and post responses for each question.
    """

    def check_correct(row, resp_col, ans_col):
        resp = row[resp_col]
        ans = row[ans_col]
        if pd.isnull(resp) or pd.isnull(ans):
            return np.nan
        return int(str(ans).lower() in str(resp).lower())

    # For each question (tag, text), add correctness columns for pre and post
    qb = questions_both.copy()
    for tag in qb.columns.get_level_values(1).unique():
        qtext = qb.columns[qb.columns.get_level_values(1) == tag][0][2]
        pre_col = ("response_pre", tag, qtext)
        post_col = ("response_post", tag, qtext)
        ans_col = ("answer", tag, qtext)
        qb[("correct_pre", tag, qtext)] = qb.apply(
            lambda row: check_correct(row, pre_col, ans_col), axis=1
        )
        qb[("correct_post", tag, qtext)] = qb.apply(
            lambda row: check_correct(row, post_col, ans_col), axis=1
        )

    # Reorder columns so that each correct column is right after its response
    def reorder_columns(df):
        cols = []
        for col in df.columns:
            if col[0] in ["response_pre", "response_post"]:
                cols.append(col)
                correct_col = (
                    "correct_pre" if col[0] == "response_pre" else "correct_post",
                    col[1],
                    col[2],
                )
                if correct_col in df.columns:
                    cols.append(correct_col)
            elif col[0] == "answer":
                cols.append(col)
        return df[cols]

    return reorder_columns(qb)
