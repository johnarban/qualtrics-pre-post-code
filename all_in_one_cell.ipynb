{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6afa2833",
   "metadata": {},
   "source": [
    "# CosmicDS Survey Data Analysis\n",
    "_Some documentation_ **Courtesy of Copilot**\n",
    "\n",
    "This notebook processes pre- and post-survey data for CosmicDS. It loads, cleans, merges, and summarizes survey responses.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Guidance for Exploring Data, Relations, and Correlations in This Notebook\n",
    "\n",
    "This notebook is designed to help you analyze and explore pre- and post-survey data from the CosmicDS project. Below are some tips and guidance for using the notebook to explore relationships and correlations in your data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Understanding the Data Structure**\n",
    "\n",
    "- **DataFrames:** The main data is stored in pandas DataFrames (`df_pre`, `df_post`, `df_combined`).\n",
    "- **Questions Metadata:** The `questions` DataFrame contains metadata about each survey question, including type (Likert, content, demographic, etc.).\n",
    "- **Merged Data:** `df_combined` merges pre- and post-survey responses, along with class and educator info.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Exploring Survey Responses**\n",
    "\n",
    "- **Group By:** Use `groupby` to aggregate responses by class, educator, question, or demographic group.\n",
    "    ```python\n",
    "    # Example: Group by class and question\n",
    "    df_combined.groupby(['class_name', 'question'])['response_post'].value_counts()\n",
    "    ```\n",
    "- **Filtering:** Filter DataFrames to focus on specific questions, classes, or student groups.\n",
    "    ```python\n",
    "    # Example: Filter for a specific question\n",
    "    df_combined[df_combined['question'] == 'Your Question Text']\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989857ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def pprint(pandas_dataframe: pd.DataFrame, max_width=1000, **kwargs):\n",
    "    return HTML(\n",
    "        \"\\n\".join(\n",
    "            Table.from_pandas(pandas_dataframe).pformat(max_width=max_width, **kwargs)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "import class_analytics_utils as cau\n",
    "\n",
    "reload(cau)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "USE_FRESH_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47464c69",
   "metadata": {},
   "source": [
    "## Data Loading and Setup\n",
    "- Loads survey data from local CSV or API.\n",
    "- Cleans up question descriptions and aligns columns between pre and post.\n",
    "- Drops unnecessary columns.\n",
    "- Uses pandas DataFrames for all tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qualtrics_keys import (\n",
    "    survey_id_2025_post,\n",
    "    survey_id_2025_pre,\n",
    "    survey_id_2024_post,\n",
    "    survey_id_2024_pre,\n",
    ")\n",
    "\n",
    "\n",
    "id_column = \"Intro information_1\"\n",
    "\n",
    "# pre_response = cau.load_pre_data(from_api=False)\n",
    "file_exists = os.path.exists(\"2025_pre_response.csv\")\n",
    "\n",
    "pre_response = cau.load_data(\n",
    "    from_api=USE_FRESH_DATA,\n",
    "    survey_id=survey_id_2025_pre,\n",
    "    filename=\"2025_pre_response.csv\",\n",
    ")\n",
    "header_pre, description_pre, data_pre = cau.parse_response(pre_response)\n",
    "\n",
    "\n",
    "post_response = cau.load_data(\n",
    "    from_api=USE_FRESH_DATA,\n",
    "    survey_id=survey_id_2025_post,\n",
    "    filename=\"2025_post_response.csv\",\n",
    ")\n",
    "header_post, description_post, data_post = cau.parse_response(post_response)\n",
    "\n",
    "\n",
    "# updates some columns to match properly, and drop columns in the list\n",
    "drop_columns = [\"CosmicDS Pre-Survey - Click to write Form Field 4\"]\n",
    "cau.column_cleanup(\n",
    "    header_pre,\n",
    "    description_pre,\n",
    "    data_pre,\n",
    "    header_post,\n",
    "    description_post,\n",
    "    data_post,\n",
    "    drop_columns=drop_columns,\n",
    ")\n",
    "\n",
    "for question in description_post:\n",
    "    # makes question tags match between matching questions\n",
    "    # probably don't need to do this since we do everything based on the question text\n",
    "    cau.get_pre_header_for_post_question(\n",
    "        question,\n",
    "        header_pre,\n",
    "        description_pre,\n",
    "        header_post,\n",
    "        description_post,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# print out the number of records\n",
    "print(\"\\n\\n\\n Number of records in Pre/Post survey\")\n",
    "print(\"Pre-survey records: \", len(data_pre))\n",
    "print(\"Post-survey records: \", len(data_post))\n",
    "\n",
    "USE_FRESH_DATA = False # set to false to use the data from the last run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98329c9e",
   "metadata": {},
   "source": [
    "## DataFrame Construction and Cleaning\n",
    "- Converts wide-format survey data to long format using `pd.melt`.\n",
    "- Merges question metadata.\n",
    "- Processes and combines demographic and reflection questions.\n",
    "- Converts Likert responses to numeric for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53040fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import qualtrics_data_fixes as qdf\n",
    "    # qualtrics_data_fixes contains two functions to modify the data\n",
    "    # These functions should not be uploaded to the repo\n",
    "\n",
    "    reload(qdf)\n",
    "    qdf.modify_pre_data(data_pre)\n",
    "    qdf.modify_post_data(data_post)\n",
    "except ImportError:\n",
    "    print(\"No qualtrics_data_fixes.py file found.  Applying no fixes.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f956603",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(cau)\n",
    "id_column = description_post[header_post.index(\"Intro information_1\")]\n",
    "instructor_column = \"CosmicDS Pre-Survey - Instructor's Last Name\"\n",
    "section_column = 'CosmicDS Pre-Survey - Course/Section'\n",
    "\n",
    "\n",
    "df_pre, df_post = cau.create_initial_dataframes(\n",
    "    header_pre,\n",
    "    description_pre,\n",
    "    data_pre,\n",
    "    header_post,\n",
    "    description_post,\n",
    "    data_post,\n",
    "    id_column,\n",
    ")\n",
    "assert df_pre[id_column].is_unique, \"Duplicate values found in id_column of df_pre\"\n",
    "assert df_post[id_column].is_unique, \"Duplicate values found in id_column of df_post\"\n",
    "\n",
    "\n",
    "def process_likert_columns(df, questions):\n",
    "    # convert the likert columns to numeric\n",
    "    for col in df.columns:\n",
    "        if col in questions[\"question\"].values:\n",
    "            if questions.loc[questions[\"question\"] == col][\"is_likert\"].values[0]:\n",
    "                df[col] = df[col].apply(lambda x: cau.convert_likert_to_numeric(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "pre_column_filter = lambda arr: [p for p in arr if (p in df_pre.columns)]\n",
    "post_column_filter = lambda arr: [p for p in arr if (p in df_post.columns)]\n",
    "\n",
    "\n",
    "def drop_columns(columns):\n",
    "    df_pre.drop(columns=pre_column_filter(columns), inplace=True)\n",
    "    df_post.drop(columns=post_column_filter(columns), inplace=True)\n",
    "\n",
    "\n",
    "questions = cau.create_question_dataframe(\n",
    "    df_pre, df_post, header_pre, description_pre, header_post, description_post\n",
    ")\n",
    "cau.add_likert_columns(questions, df_pre, df_post)\n",
    "df_pre = process_likert_columns(df_pre, questions)\n",
    "df_post = process_likert_columns(df_post, questions)\n",
    "\n",
    "removed_questions = []\n",
    "\n",
    "questions, removed = cau.process_parent_questions(questions, df_pre, df_post)\n",
    "removed_questions.extend(removed)\n",
    "drop_columns(removed_questions)\n",
    "\n",
    "questions, removed = cau.process_race_questions(questions, df_pre, df_post)\n",
    "removed_questions.extend(removed)\n",
    "drop_columns(removed_questions)\n",
    "\n",
    "questions, removed = cau.process_gender_questions(questions, df_pre, df_post)\n",
    "removed_questions.extend(removed)\n",
    "drop_columns(removed_questions)\n",
    "\n",
    "questions, removed = cau.process_school_questions(questions, df_pre, df_post)\n",
    "removed_questions.extend(removed)\n",
    "drop_columns(removed_questions)\n",
    "\n",
    "questions, removed = cau.process_esl_questions(questions, df_pre, df_post)\n",
    "removed_questions.extend(removed)\n",
    "drop_columns(removed_questions)\n",
    "\n",
    "questions, removed = cau.process_activity_questions(questions, df_pre, df_post)\n",
    "removed_questions.extend(removed)\n",
    "drop_columns(removed_questions)\n",
    "\n",
    "questions, removed = cau.process_confused_questions(questions, df_pre, df_post)\n",
    "removed_questions.extend(removed)\n",
    "drop_columns(removed_questions)\n",
    "\n",
    "cau.add_content_question_meta(questions, description_pre, description_post)\n",
    "cau.add_likert_meta(questions)\n",
    "\n",
    "# Here we are going to add in the meta data for the questions\n",
    "# While df_pre and df_post have the questions as columns\n",
    "# this step performs a \"melt\" taking the wide format and making it long\n",
    "# So that each student entry now has a row for each question & it's response\n",
    "# the responses are under the column \"response\" and the question is under \"question\"\n",
    "id_columns = list(questions.groupby(\"question_category\").get_group(\"intro\")[\"question\"])\n",
    "\n",
    "\n",
    "pre_only_questions = [\n",
    "    \"Parent 1 Education\",\n",
    "    \"Parent 1 Gender\",\n",
    "    \"Parent 2 Education\",\n",
    "    \"Parent 2 Gender\",\n",
    "    \"Student Race\",\n",
    "    \"Student Gender\",\n",
    "    \"Student School Level\",\n",
    "    \"Student ESL\",\n",
    "]\n",
    "\n",
    "df_pre_merged, df_post_merged = cau.merge_questions_into_dataframes(\n",
    "    df_pre, df_post, questions, id_column, id_columns, pre_only_questions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9fda1",
   "metadata": {},
   "source": [
    "## Matching and Merging Pre/Post Data\n",
    "- Matches students by ID between pre and post surveys.\n",
    "- Merges responses on ID and question.\n",
    "- Adds class and educator info from the database.\n",
    "- Calculates response rates and basic stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b87c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre-test merged records: \", len(df_pre_merged))\n",
    "# display(pprint(df_pre_merged.head(5), html=True))\n",
    "print(\"Post-test merged records: \", len(df_post_merged))\n",
    "# display(pprint(df_post_merged.head(5), html=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line counts the number of questions in each (question_category, group)\n",
    "# combination by exploding the 'groups' column and grouping by 'question_category' and 'groups'.\n",
    "print(\"Number of questions in each question qategory and group:\")\n",
    "questions.explode(\"groups\").groupby([\"question_category\", \"groups\"]).count().rename(\n",
    "    {\"question\": \"count\"}, axis=1\n",
    ")[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e6c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(cau)\n",
    "matched_ids = list(\n",
    "    set(df_pre_merged[id_column]).intersection(df_post_merged[id_column])\n",
    ")\n",
    "orphraned_pre = list(\n",
    "    set(df_pre_merged[id_column]) - set(df_post_merged[id_column])\n",
    ")\n",
    "orphraned_post = list(\n",
    "    set(df_post_merged[id_column]) - set(df_pre_merged[id_column])\n",
    ")\n",
    "\n",
    "print(\"Number of orphaned pre ids:\", len(orphraned_pre))\n",
    "print(\"Number of orphaned post ids:\", len(orphraned_post))\n",
    "\n",
    "matched_idx = df_pre_merged[df_pre_merged[id_column].isin(matched_ids)].index\n",
    "matched_idx_post = df_post_merged[df_post_merged[id_column].isin(matched_ids)].index\n",
    "print(\"Number of matched ids:\", len(matched_ids))\n",
    "\n",
    "# merging on the id_column and the questions\n",
    "# By doing an outer join,\n",
    "df_combined = pd.merge(\n",
    "    df_pre_merged,\n",
    "    df_post_merged,\n",
    "    on=[id_column] + questions.columns.tolist(),\n",
    "    how=\"outer\",  # inner join to keep only matching rows\n",
    "    suffixes=(\"_pre\", \"_post\"),\n",
    ")\n",
    "\n",
    "def notnull(value):\n",
    "    return value not in ['(empty)', '', None, np.nan]\n",
    "def has_pre_and_post_respose(row):\n",
    "    return notnull(row[\"response_pre\"]) and notnull(row[\"response_post\"])\n",
    "\n",
    "# matched_ids = (df_combined\n",
    "#                .drop_duplicates(subset=[id_column])\n",
    "#                .apply(lambda row: has_pre_and_post_respose(row), axis=1)\n",
    "# )\n",
    "\n",
    "\n",
    "df_combined[\"matched\"] = df_combined[id_column].apply(\n",
    "    lambda x: x in matched_ids\n",
    ")\n",
    "\n",
    "df_combined[\"orphaned_pre\"] = df_combined[id_column].apply(\n",
    "    lambda x: x in orphraned_pre\n",
    ")\n",
    "df_combined[\"orphaned_post\"] = df_combined[id_column].apply(\n",
    "    lambda x: x in orphraned_post\n",
    ")\n",
    "\n",
    "c = list(\n",
    "    [id_column] + pre_only_questions\n",
    "    + ['matched', 'orphaned_pre', 'orphaned_post']\n",
    "    + questions.columns.tolist()\n",
    "    + [\"response_pre\", \"correct_pre\", \"response_post\", \"correct_post\"]\n",
    "    + [c + \"_pre\" for c in id_columns[1:]]\n",
    "    + [c + \"_post\" for c in id_columns[1:]]\n",
    ")\n",
    "\n",
    "df_combined = df_combined[c]\n",
    "# df_combined = df_combined.rename(columns={\"question\": \"question\"})\n",
    "\n",
    "\n",
    "# get the appropriate classroom data\n",
    "df_combined = cau.get_class_info(df_combined, id_column)\n",
    "\n",
    "print(f\"There are {len(df_combined)} rows in the combined dataframe\")\n",
    "print(f\"There are {len(df_combined[df_combined['matched']])} rows with matched ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3834a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will print out all of the students for examination\n",
    "df_combined[['CosmicDS Pre-Survey - CosmicDS Student ID number (Ask your instructor if you do not know this)',\n",
    "            \"CosmicDS Pre-Survey - Instructor's Last Name_pre\",\n",
    "       'CosmicDS Pre-Survey - Course/Section_pre',\n",
    "       \"CosmicDS Pre-Survey - Instructor's Last Name_post\",\n",
    "       'CosmicDS Pre-Survey - Course/Section_post', 'student_id', 'class_name',\n",
    "       'class_id', 'Educator']].head(10) #.to_excel('studentsclassroom.xlsx', index=False, engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic stats\n",
    "print(f\"Unique IDs in combined: {len(df_combined[id_column].unique())}\")\n",
    "print(f\"Unique IDs in pre-survey: {len(df_pre[id_column].unique())}\")\n",
    "print(f\"Unique IDs in post-survey: {len(df_post[id_column].unique())}\")\n",
    "print(f\"Unique IDs in merged pre-survey: {len(df_pre_merged[id_column].unique())}\")\n",
    "print(f\"Unique IDs in merged post-survey: {len(df_post_merged[id_column].unique())}\")\n",
    "\n",
    "missing_in_pre = set(df_post_merged[id_column]) - set(df_pre_merged[id_column])\n",
    "missing_in_post = set(df_pre_merged[id_column]) - set(df_post_merged[id_column])\n",
    "\n",
    "print(f\"IDs in post but not in pre: {len(missing_in_pre)}\")\n",
    "print(f\"IDs in pre but not in post: {len(missing_in_post)}\")\n",
    "print(\"List of IDs in post but not in pre:\", missing_in_pre)\n",
    "print(\"List of IDs in pre but not in post:\", missing_in_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609be248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1 row per student view of the data for explorting to excel\n",
    "questions_both = df_combined.pivot(\n",
    "    index=id_column,\n",
    "    columns=[\"question\"],\n",
    "    values=[\"response_pre\", \"response_post\"],\n",
    "    # aggfunc=\"first\"\n",
    ")\n",
    "questions_both.columns = ['++'.join(map(str, col)).strip() for col in questions_both.columns]\n",
    "\n",
    "# Merge back the rest of the non-pre/post q columns (demographics, etc)\n",
    "questions_both = pd.merge(\n",
    "    df_combined.drop_duplicates(subset=[id_column]).set_index(id_column),\n",
    "    questions_both,\n",
    "    on=id_column,\n",
    "    how=\"left\"\n",
    ")\n",
    "# reset ++ columns to tuples\n",
    "questions_both.columns = [\n",
    "    tuple(col.split(\"++\")) if \"++\" in col else (col, \"\") for col in questions_both.columns\n",
    "]\n",
    "# # multi-index columns where appropriate\n",
    "questions_both.columns = pd.MultiIndex.from_tuples([\n",
    "    (col, '') if not isinstance(col, tuple) else col  # promote strings to (col, '')\n",
    "    for col in questions_both.columns\n",
    "])\n",
    "\n",
    "questions_both.to_excel(\n",
    "    \"2025_combined_questions_row.xlsx\",\n",
    "    index=True,\n",
    "    # engine=\"openpyxl\",\n",
    "    sheet_name=\"2025_combined\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response Summary\")\n",
    "response_summary = (\n",
    "    df_combined.groupby(\"both\")\n",
    "    .get_group(True)\n",
    "    .drop_duplicates(subset=[id_column])\n",
    "    .groupby([\"class_name\", \"Educator\", \"class_id\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"response_pre\": lambda x: x.notnull().sum(),\n",
    "            \"response_post\": lambda x: x.notnull().sum(),\n",
    "            \"matched\": lambda x: x.sum(),\n",
    "            \"orphaned_pre\": lambda x: x.sum(),\n",
    "            \"orphaned_post\": lambda x: x.sum(),\n",
    "        }\n",
    "    )\n",
    "    .assign(post_response_rate=lambda df: df[\"matched\"] / (df[\"response_pre\"]))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Overall response rate: {response_summary['post_response_rate'].mean() * 100:.2f}%\")\n",
    "response_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48cdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matched totals\")\n",
    "(df_combined\n",
    " .groupby('Educator')\n",
    " .apply(\n",
    "    lambda group: group.drop_duplicates(subset=[id_column])[\"matched\"].sum()\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class Summary for Content (Science Questions)\")\n",
    "print(\"# of of indivual questions or percentage which are correct\")\n",
    "class_score_summary = (\n",
    "    df_combined.groupby(\"both\")\n",
    "    .get_group(True)\n",
    "    .groupby([\"question_category\"]).get_group(\"science\")\n",
    "    .groupby([\"class_name\", \"Educator\", \"class_id\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"response_pre\": lambda x: x.notnull().sum(),\n",
    "            \"response_post\": lambda x: x.notnull().sum(),\n",
    "            \"matched\": lambda x: x.sum(),\n",
    "            \"correct_pre\": lambda x: f\"{x.sum() / x.notnull().sum():.0%}\",\n",
    "            \"correct_post\": lambda x: f\"{x.sum() / x.notnull().sum():.0%}\",\n",
    "        }\n",
    "    )\n",
    "    .assign(post_response_rate=lambda df: df[\"matched\"] / (df[\"response_pre\"]))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "# Define styles for pandas formatting\n",
    "def style_class_score_summary(row):\n",
    "    # (modified from) Generated by Copilot\n",
    "    if row['response_post'] == 0:\n",
    "        return ['background-color: #FFCCCC; color: black'] * len(row)  # Pale blue for no post\n",
    "    elif float(row['correct_post'].strip('%')) / 100 < 0.35:\n",
    "        return ['background-color: #F00' if col == 'correct_post' else '' for col in row.index]  # Pale red for < 30%\n",
    "    elif float(row['correct_post'].strip('%')) / 100 > 0.75:\n",
    "        return ['background-color: #CCFFCC; color: black' if col == 'correct_post' else '' for col in row.index]  # Pale green for > 80%\n",
    "    else:\n",
    "        return [''] * len(row)\n",
    "\n",
    "# Apply the styles\n",
    "styled_class_score_summary = class_score_summary.style.apply(style_class_score_summary, axis=1)\n",
    "\n",
    "styled_class_score_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_teacher = df_combined.groupby(\"Educator\")\n",
    "by_class = df_combined.groupby(\"class_name\")\n",
    "\n",
    "\n",
    "by_teacher_class = df_combined.groupby([\"Educator\", \"class_name\"])\n",
    "print(\"Teacher and Class Groups\")\n",
    "for name, group in by_teacher_class:\n",
    "    print(f\"Group: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6d93c",
   "metadata": {},
   "source": [
    "# Selecting a Group\n",
    "\n",
    "```python\n",
    "# To create a group, you can use the `groupby` method on the DataFrame.\n",
    "# For example, to group by \"Educator\":\n",
    "grouped = df_combined.groupby(\"Educator\")\n",
    "\n",
    "# To select a specific group, use the `get_group` method:\n",
    "selected_group = grouped.get_group(\"Educator Name\")\n",
    "\n",
    "# To view the available keys in the group:\n",
    "print(\"Available group keys:\", grouped.groups.keys())\n",
    "```\n",
    "\n",
    "For a specifier which could have multiple values, like race or gender, we need to \"explode\" the identifier. (This means students can be counted in multiple groups). For example\n",
    "\n",
    "```python\n",
    "by_race = df_combined.explode(\"Student Race\").groupby(\"Student Race\")\n",
    "print(by_race.groups.keys()) # view valid keys\n",
    "```\n",
    "\n",
    "You can even group at multiple levels, so by Teacher and Class\n",
    "```python\n",
    "by_teacher_class = df_combined.groupby([\"Educator\", \"class_name\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff27ae2",
   "metadata": {},
   "source": [
    "## Valid columns for selecting by\n",
    "- 'CosmicDS Pre-Survey - CosmicDS Student ID number (Ask your instructor if you do not know this)' (can select it with `id_column`)\n",
    "- Parent Demographics\n",
    "    - 'Parent 1 Education'\n",
    "    - 'Parent 1 Gender'\n",
    "    - 'Parent 2 Education'\n",
    "    - 'Parent 2 Gender'\n",
    "- Student Demographics\n",
    "    - 'Student Race'\n",
    "    - 'Student Gender'\n",
    "    - 'Student School Level'\n",
    "    - 'Student ESL'\n",
    "- 'matched'\n",
    "    - Only pre-post matched students\n",
    "    - Only use `get_group(True)`, `False` returns nonsense\n",
    "- 'orphaned_pre'\n",
    "- 'orphaned_post'\n",
    "- 'question'\n",
    "    - used for applying\n",
    "- Classroom info\n",
    "    - 'class_name'\n",
    "    - 'class_id'\n",
    "    - 'Educator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select only matched responses, use only the matched group\n",
    "# unmatched post-feedback will not be included\n",
    "selected_group = df_combined.groupby(\"matched\").get_group(True) \n",
    "\n",
    "# To selection an specific educator\n",
    "# selected_group = df_combined.groupby(\"Educator\").get_group((\"Educator Name\"))\n",
    "\n",
    "# groups = df_combined.explode(\"Student Race\").groupby(\"Student Race\")\n",
    "\n",
    "# print(\"Possible Groups:\", list(groups.groups.keys()),'\\n')\n",
    "# selected_group  = groups.get_group(\"Black or African American\")\n",
    "\n",
    "# no group\n",
    "# selected_group = by_teacher.get_group((\"Educator Names\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbb1f5d",
   "metadata": {},
   "source": [
    "Note on effect sizes for binary data (the content group)\n",
    "\n",
    "| **#** | **Name**                               | **Formula / Description**                                                                   | **Proper Use**                                         | **Pairing Assumed?** |\n",
    "| ----- | -------------------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------ | -------------------- |\n",
    "| 1     | **Cohen’s *h***                        | $h = 2 \\cdot \\arcsin(\\sqrt{p_2}) - 2 \\cdot \\arcsin(\\sqrt{p_1})$                             | Standardized effect size for proportions               | ❌ No                 |\n",
    "| 2     | **Standardized Prop. Difference**      | $z = \\frac{p_2 - p_1}{\\sqrt{ \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2} }}$            | Signal-to-noise effect size or z-test                  | ❌ No                 |\n",
    "| 3     | **Two-sample z-test for proportions**  | $z = \\frac{p_2 - p_1}{\\sqrt{P(1-P)(1/n_1 + 1/n_2)}}, \\quad P = \\frac{c_1 + c_2}{n_1 + n_2}$ | Hypothesis test for group proportion difference        | ❌ No                 |\n",
    "| 4     | **Binary Cohen’s *d*** *(your method)* | $d = \\frac{p_2 - p_1}{\\sqrt{ \\text{pooled } \\left( \\frac{p(1 - p)}{n} \\right) }}$           | Cohen-style standardized binary ES                     | ❌ No                 |\n",
    "| 5     | **McNemar’s Test**  (one-sided improvement)        | $p = \\text{binomtest}(k = \\text{True→False},\\ n = \\text{True→False} + \\text{False→True},\\ p = 0.5, \\text{alt=greater})$                        | Tests for directional change in **paired** binary data | ✅ Yes                |\n",
    "\n",
    "The `effect_size` we report is the \"Binary Cohen's d\", it is a version of the standard Cohen's d, where the variance used is for a binomial distributions. This value will diverge quickly for large N, basically because the variance gets really small\n",
    "\n",
    "**Cohen's h** is the more _official_ statistic for this comparison, and is meant be interpreted on the same scaling as a typcial cohen's d. it is better behaved, not exploding for small variance\n",
    "\n",
    "Not listed here is the result of binomtest, which is precise for binary data and small numbers. The p-value and percentage difference are the best measures  b=False→True, c=True→False\n",
    "\n",
    "The **most precise** statistic is McNemar's\n",
    "\n",
    "\n",
    "## Note on P-values\n",
    "- `mcnemar_p` tests **whether more people improved than regressed**\n",
    "- `p_value` is an overall p-value and only tests **are overall pre/post proportions different?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccac87d",
   "metadata": {},
   "source": [
    "## Values returned for multiple choice data\n",
    "\n",
    "| **Parameter**                                         | **Description**                                                                   |\n",
    "| ----------------------------------------------------- | --------------------------------------------------------------------------------- |\n",
    "| `percent_correct_pre`                                 | Percent of students who answered correctly on the pre-test                        |\n",
    "| `percent_correct_post`                                | Percent of students who answered correctly on the post-test                       |\n",
    "| `summary`                                             | Summary sentence describing overall change and statistical significance           |\n",
    "| `effect_size`                                         | Binary Cohen’s *d*-style effect size based on raw count difference                |\n",
    "| `p_value`                                             | Binomial test p-value for change in total correctness from pre to post            |\n",
    "| `cohens_h`                                            | Cohen’s *h*: standardized difference in proportions using arcsin-sqrt transform   |\n",
    "| `mcnemar_statistic`                                   | Proportion of changers who improved (f->t / (f->t + t->f))    |\n",
    "| `mcnemar_p`                                           | One-sided McNemar p-value: tests if improvements exceed regressions               |\n",
    "| `discordant_summary`                                  | Summary sentence for direction and significance of paired changes                 |\n",
    "| `standardized_difference`                             | Z-like standardized difference in proportions using unpooled binomial variance    |\n",
    "| `z_statistic`                                         | Z-statistic for difference in proportions assuming equal proportions under null   |\n",
    "| `false_to_true_p_value`                               | Binomial test for students who improved out of those who were initially incorrect |\n",
    "| `sqrt_pooled_variance`                                | Square root of pooled binomial variance (used in effect size calculation)         |\n",
    "| `pre_to_post_change`                                  | Net change in number of correct answers (post − pre)                              |\n",
    "| `pre_to_post_change_percent`                          | Net change as a percent of total students                                         |\n",
    "| `direction`                                           | Sign of net change: +1 (improvement), 0, or -1 (decline)                          |\n",
    "| `confused_count`                                      | Number of students who regressed (correct → incorrect)                            |\n",
    "| `confused`                                            | Percent of previously correct students who got it wrong post-test                 |\n",
    "| `confused_p_value`                                    | Binomial test for whether regression rate is higher than chance                   |\n",
    "| `n_discordant`                                        | Number of students who changed responses (correct → incorrect or vice versa)      |\n",
    "| `n_change_to_true`                                    | Number of students who improved (incorrect → correct)                             |\n",
    "| `tag_pre`, `tag_post`                                 | Metadata: tags associated with pre and post items                                 |\n",
    "| `pre_correct`, `post_correct`                         | Count of correct answers pre and post                                             |\n",
    "| `pre_count`, `post_count`                             | Total number of pre and post responses                                            |\n",
    "| `group`                                               | Group identifier(s), e.g., demographic label or class group                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(cau)\n",
    "\n",
    "def group_analysis(selected_group):\n",
    "    questions_only_in_pre = selected_group.groupby([\"both\", \"in_pre\", \"in_post\"]).get_group(\n",
    "        (False, True, False)\n",
    "    )\n",
    "    questions_only_in_post = selected_group.groupby(\n",
    "        [\"both\", \"in_pre\", \"in_post\"]\n",
    "    ).get_group((False, False, True))\n",
    "\n",
    "\n",
    "    drop_questions = [\n",
    "        \"Astro Content MLO 1b\",\n",
    "        \"Data Skills MLO 3c\",\n",
    "        \"Data Skills likert MLO 3c\",\n",
    "        \"Nature of science MLO 2a\",\n",
    "        \"STEM interest & Identity MLO 1a\",\n",
    "        \"Score\",\n",
    "    ]\n",
    "    student_info = cau.create_stats_for_pre(questions_only_in_pre, pre_only_questions) # a list of series\n",
    "    print(\"Available Items: \", [s.index.name for s in student_info])\n",
    "    post_reflection_summary = cau.create_post_reflection_summary(questions_only_in_post, id_column)\n",
    "\n",
    "\n",
    "    # Get the list of matched questions\n",
    "    # matched = matched rows\n",
    "    # both = question appears in both pre and post\n",
    "    try:\n",
    "        matched_responses = (\n",
    "            selected_group.groupby([\"matched\", \"both\"])\n",
    "            .get_group((True, True))\n",
    "            .groupby(\"question_category\")\n",
    "        )\n",
    "    except KeyError:\n",
    "        print(\"No matched responses found to create matched_responses\")\n",
    "        return None, None, None, None, questions, student_info, post_reflection_summary, None\n",
    "    print(matched_responses.groups.keys())\n",
    "\n",
    "    # get the likert questions\n",
    "    likert_group = matched_responses.get_group(\"likert\")\n",
    "\n",
    "    # get the multiple choice questions\n",
    "    content_group = matched_responses.get_group(\"science\")\n",
    "\n",
    "    # group responses by questions\n",
    "    likert_questions = likert_group.groupby(\"question\")\n",
    "    content_questions = content_group.groupby(\"question\")\n",
    "\n",
    "    # apply our summary to each group/question\n",
    "    likert_out = likert_questions.apply(cau.create_likert_summary)\n",
    "\n",
    "    # reset index to be tag_pre and put the current index in 'question'\n",
    "    likert_out.reset_index(inplace=True)\n",
    "    likert_out.set_index(\"tag_pre\", inplace=True)\n",
    "    likert_out.sort_index(key=lambda x: list(map(cau.q_to_num, x)), inplace=True)\n",
    "    # removing the question pre-amble.\n",
    "    likert_out[\"question\"] = likert_out[\"question\"].apply(\n",
    "        lambda x: x.split(\"? -\", maxsplit=1)[-1].strip()\n",
    "    )\n",
    "\n",
    "    # apply summary to each class of likert questions\n",
    "    likert_groups_out = (\n",
    "        matched_responses.get_group(\"likert\")\n",
    "        .explode(\"likert_group\")\n",
    "        .groupby(\"likert_group\")\n",
    "        .apply(lambda gr: cau.create_likert_summary(gr, force_improved=True))\n",
    "    )\n",
    "\n",
    "\n",
    "    content_out = content_questions.apply(cau.create_content_summary)\n",
    "    content_out.reset_index(inplace=True)\n",
    "    content_out.set_index(\"tag_pre\", inplace=True)\n",
    "    content_out.sort_index(key=lambda x: list(map(cau.q_to_num, x)), inplace=True)\n",
    "\n",
    "\n",
    "    content_group_summary = (\n",
    "        content_group.explode(\"groups\").groupby(\"groups\").apply(cau.create_content_summary)\n",
    "    )\n",
    "    return (\n",
    "        content_group_summary, \n",
    "        likert_groups_out, \n",
    "        likert_out, \n",
    "        content_out, \n",
    "        questions, \n",
    "        student_info, \n",
    "        post_reflection_summary, \n",
    "        matched_responses\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006351e",
   "metadata": {},
   "source": [
    "## Summarizing and Exporting Results\n",
    "- Groups responses by question, class, and educator.\n",
    "- Summarizes Likert and content (multiple choice) questions.\n",
    "- Writes all summary tables and raw data to an Excel workbook with multiple sheets.\n",
    "- For post_reflection_summary, each pd.Series is written as a block to a single sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "def create_workbook(filename = None, overwrite = False, summary = (None, None, None, None, None, None, None, None)):\n",
    "    if all(\n",
    "        [x is None for x in summary]\n",
    "        ):\n",
    "        raise ValueError(\"Summary data is None. Please provide summary data.\")\n",
    "    \n",
    "    content_group_summary, likert_groups_out, likert_out, content_out, questions, student_info, post_reflection_summary, matched_responses = summary\n",
    "    # create a two sheet workbook with likert_out and content_out\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb[\"Sheet\"])\n",
    "    \n",
    "    if content_group_summary is not None:\n",
    "        # create a sheet for content_group_summary\n",
    "        ws5 = wb.create_sheet(\"Content Group Summary\")\n",
    "        ws5.title = \"Content Group Summary\"\n",
    "        for r in dataframe_to_rows(content_group_summary, index=True, header=True):\n",
    "            ws5.append(r)\n",
    "\n",
    "    \n",
    "    if likert_groups_out is not None:\n",
    "        ws7 = wb.create_sheet(\"Likert Group Summary\")\n",
    "        ws7.title = \"Likert Group Summary\"\n",
    "        for r in dataframe_to_rows(likert_groups_out, index=True, header=True):\n",
    "            ws7.append(r)\n",
    "\n",
    "    \n",
    "    if likert_out is not None:\n",
    "        # create a sheet for likert_out\n",
    "        ws1 = wb.create_sheet(\"Lickert Summary\")\n",
    "        ws1.title = \"Lickert Questions\"\n",
    "        for r in dataframe_to_rows(likert_out, index=False, header=True):\n",
    "            ws1.append(r)\n",
    "\n",
    "    if content_out is not None:\n",
    "        ws2 = wb.create_sheet(\"Content Questions Summary\")\n",
    "        ws2.title = \"Content Questions\"\n",
    "        for r in dataframe_to_rows(content_out, index=False, header=True):\n",
    "            ws2.append(r)\n",
    "    \n",
    "    if matched_responses is not None:\n",
    "        likert_both = (\n",
    "            matched_responses.get_group(\"likert\")\n",
    "            .pivot(\n",
    "                index=[id_column],\n",
    "                columns=[\"tag_pre\", \"question\"],\n",
    "                values=[\"response_pre\", \"response_post\"],\n",
    "            )\n",
    "            .apply(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "        questions_both = matched_responses.get_group(\"science\").pivot(\n",
    "            index=[id_column],\n",
    "            columns=[\"tag_pre\", \"question\"],\n",
    "            values=[\"response_pre\", \"response_post\", \"answer\"],\n",
    "        )\n",
    "        questions_both = cau.add_answer_column(questions_both)\n",
    "\n",
    "        # create a sheet for likert_both\n",
    "        ws3 = wb.create_sheet(\"Lickert Data\")\n",
    "        ws3.title = \"Lickert Data\"\n",
    "        for r in dataframe_to_rows(likert_both, index=True, header=True):\n",
    "            ws3.append(r)\n",
    "\n",
    "\n",
    "        # create a sheet for questions_both\n",
    "        ws4 = wb.create_sheet(\"Content Question Data\")\n",
    "        ws4.title = \"Content Question Data\"\n",
    "        for r in dataframe_to_rows(questions_both, index=True, header=True):\n",
    "            ws4.append(r)\n",
    "\n",
    "\n",
    "    # create a page for the questions\n",
    "    ws6 = wb.create_sheet(\"question\")\n",
    "    ws6.title = \"question\"\n",
    "    excel_questions = questions.copy()\n",
    "    # drop likert_group column\n",
    "    excel_questions.drop(columns=[\"likert_group\"], inplace=True)\n",
    "    excel_questions[\"groups\"] = excel_questions[\"groups\"].apply(\n",
    "        lambda x: \", \".join(list(x)) if len(x) > 0 else None\n",
    "    )\n",
    "    for r in dataframe_to_rows(excel_questions, index=False, header=True):\n",
    "        ws6.append(r)\n",
    "\n",
    "\n",
    "    ws8 = wb.create_sheet(\"Full Combined Data\")\n",
    "    ws8.title = \"Full Combined Data\"\n",
    "    for r in dataframe_to_rows(df_combined, index=False, header=True):\n",
    "        # coercer row into string\n",
    "        ws8.append([str(x) for x in r])\n",
    "\n",
    "\n",
    "    # student_info\n",
    "    ws9 = wb.create_sheet(\"Student Info\")\n",
    "    ws9.title = \"Student Info\"\n",
    "    for info_frame in student_info:\n",
    "        for r in dataframe_to_rows(info_frame, index=True, header=True):\n",
    "            ws9.append(r)\n",
    "        ws9.append([])\n",
    "\n",
    "    # post_reflection_summary\n",
    "    ws10 = wb.create_sheet(\"Post Reflection Summary\")\n",
    "    ws10.title = \"Post Reflection Summary\"\n",
    "    for series in post_reflection_summary:\n",
    "\n",
    "        for r in dataframe_to_rows(pd.DataFrame(series), index=True, header=True):\n",
    "            ws10.append(r)\n",
    "        # add some empty rows\n",
    "        ws10.append([])\n",
    "\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = \"all_stats_2025.xlsx\"\n",
    "        \n",
    "    if os.path.exists(filename) and not overwrite:\n",
    "        raise FileExistsError(\n",
    "            f\"{filename} already exists. Please remove it before running this code or set overwrite=True\"\n",
    "        )\n",
    "\n",
    "    wb.save(filename)\n",
    "    print(f\"Workbook saved to {filename}\")\n",
    "    return wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = group_analysis(df_combined)\n",
    "wb = create_workbook(\n",
    "    filename=f\"all_data_summary_2025.xlsx\",\n",
    "    overwrite=True,\n",
    "    summary=summaries\n",
    ")\n",
    "print(f\"Workbook saved to all_data_summary_2025.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, selected_group in df_combined.groupby([\"Educator\"]):\n",
    "    summaries = group_analysis(selected_group)\n",
    "    wb = create_workbook(\n",
    "        filename=f\"{name}_summary_2025.xlsx\",\n",
    "        overwrite=True,\n",
    "        summary=summaries\n",
    "    )\n",
    "    print(f\"Workbook saved to {name}_summary_2025.xlsx\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cac92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
